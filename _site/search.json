[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Max Rohde",
    "section": "",
    "text": "Resume\n  \n  \n     Email\n  \n  \n     Google Scholar\n  \n  \n     Github\n  \n  \n     Twitter\n  \n  \n     LinkedIn\n  \n\n  \n  \n\n\nHello, I’m Max. I’m currently a PhD student at Vanderbilt University in the Department of Biostatistics. Currently I’m conducting research on Bayesian clinical trial designs for COVID-19 with Dr. Frank Harrell, and I’m a statistician on the ACTIV-6 trial.\n\n\n\nVanderbilt University | Nashville, TN\nPhD in Biostatistics | 2020 - Present\nCarleton College | Northfield, MN\nBA in Physics | 2013 - 2017\nBA in Geology | 2013 - 2017"
  },
  {
    "objectID": "about.html#research-work-experience",
    "href": "about.html#research-work-experience",
    "title": "Max Rohde",
    "section": "Research / Work Experience",
    "text": "Research / Work Experience\n\nORISE Fellow at FDA Division of Antivirals: Natural-language processing tools for product label data and meta-analyses of clinical trials for hepatitis C and HIV treatments.\nMIT Washington Office: Reporting on current events in science policy.\nAlbuquerque Seismological Laboratory: Effects of atmospheric noise on seismometers.\nLIGO collaboration: Magnetic transients and implications for gravitational wave astronomy.\nAcademic tutoring: teaching physics, chemistry, and mathematics to high-school and undergraduate students."
  },
  {
    "objectID": "about.html#personal",
    "href": "about.html#personal",
    "title": "Max Rohde",
    "section": "Personal",
    "text": "Personal\nIn my free time I enjoy cooking (and collecting cookbooks), playing funk guitar, and swimming."
  },
  {
    "objectID": "anim.html",
    "href": "anim.html",
    "title": "Animations",
    "section": "",
    "text": "Below are some animations I created using gganimate to help me understand concepts in statistics. The work of 3Blue1Brown and Seeing Theory have been an inspiration for me.\n\nOverfitting with Regression Splines\nRegression splines are an effective tool for fitting curves to non-linear data. However, the flexibility of splines is both a blessing and a curse. If the degrees of freedom are set too high, splines can quickly overfit a dataset – resulting in good performance on the training data but poor performance on the testing data.\nThe below animation shows training data (top left) and test data (top right), with the true data generating function as the gray dashed line. We see that the best performance on the test set is obtained at about 5 degrees of freedom, while the training error goes to zero, a clear sign of overfitting.\n\n\nVideo\n\n\n\n\nIllustrating the bias-variance tradeoff with the KNN and Least-Squares classifiers\nComparing the least-squares classifier to the KNN classifier, fit on bootstrap resamples, demonstrates the bias-variance tradeoff. The black line is the true 0.5 probability contour, the red line is the estimated 0.5 probability contour. Data from Elements of Statistical Learning.\n\n\nVideo\n\n\n\n\nExample of link function misspecification in GLM for binary outcomes\n\n\nVideo\n\n\n\n\nBayesian Inference for a Binomial Proportion\nBayesian inference can be using to quantity our uncertainty about the parameter \\(p\\) from a statistical model where the data is distributed \\(\\text{Bin}(n,p)\\). An example is the bias of a coin. In the below example, the true bias is \\(p = 0.7\\). As the data come in, we can update our posterior distribution for \\(p\\).\n\n\nVideo\n\n\n\n\nk-Nearest Neighbors Decision Boundary\nk-Nearest Neighbors is a non-parametric classification algorithm that classifies each point to the majority class of the \\(k\\) nearest points. The number of neighbors to use greatly affects the decision boundaries, as shown in the below animation.\n\n\nVideo\n\n\n\n\nSimple MCMC Animation\n\n\nVideo\n\n\n\n\nBayesian Linear Regression\nBayesian inference can also be used in linear regression. As the data is collected, we become more confident about the parameters of the linear regression model:\n\nthe slope of the linear relationship\nthe variance of the error term\nthe intercept of the linear relationship\n\nThe model was fit using Stan to do the MCMC sampling and the posterior distributions were plotted using a kernel density estimate.\n\n\nVideo\n\n\n\n\nVisualizing priors\nWhen doing Bayesian inference, it’s important to visualize what your priors are implying. For a linear regression with only an intercept and slope, a N(0,1) prior on the slope produces very different regression lines (before using data) compared to a N(0,10) prior.\n\n\nVideo\n\n\n\n\nCalculating π with Monte Carlo Estimation\nMonte Carlo estimation is a technique for solving deterministic problems by random sampling.\nFor example, you can compute π by uniform sampling within a square for \\(x \\in [-1,1], y \\in [-1,1]\\), and rejecting the points where \\(x^2 + y^2 > 1\\).\nThen \\(\\pi = \\text{(Proportion not rejected)} \\times 4\\).\n\n\nVideo\n\n\n\n\nOverfitting in Polynomial Regression\nOverfitting is a major problem when fitting complex models to few data points. As a simple example, polynomial regression can fit noise in the data rather than the true model. Here the true model is quadratic with error that is normally distributed with mean zero. As the degree of the polynomial increases, the model rapidly overfits the data. At the extreme, if the degree of the polynomial is greater than (Number of points - 1), then the fitted polynomial will pass through every data point.\n\n\nVideo\n\n\n\n\nFinding the MLE estimate of the mean for a Normal Distribution\nYou can estimate the MLE for the mean of a normal distribution given a dataset by varying the mean until the peak of the log-likelihood is reached. This visually looks like sliding around different candidate distributions until the best match is found. Once the MLE for the mean is found, the variance can be varied in the same fashion (keeping the mean fixed at the MLE) to find the MLE for the variance.\n\n\nVideo\n\n\n\n\nExample of collider bias\n\n\n\n\n\n\n\nUsing Quantile-Quantile Plots to Detect Depatures from Normality\nQuantile-quantile plots are a useful tool for assessing the fit of data to a given distribution. This animation shows Q-Q plots for t-distributed data with various degrees of freedom. We see that the Q-Q plot shows more clearly the departure from normality compared to the histogram.\n\n\nVideo\n\n\n\n\nThe German Tank Problem\nThe German Tank problem is a famous problem in statistics. During World War 2, the Allied forces used the serial numbers on German tanks to estimate the number of tanks produced. The results of this statistical analysis estimated a production of 246 tanks per month, while intelligence from spies estimated a much higher rate of around 1.400 tanks per month. After the war, German records showed that the true rate was 245 per month! The statistical evidence that the numbers of tanks was lower than expected gave the Allies motivation to attack the Western Front, leading to the fall of Berlin and the end of the war in Europe.\nWe can formulate this problem assuming the serial numbers start at 1 and are randomly sampled from the population of tanks. Let \\(X_1, X_2, \\ldots X_n\\) be a sample of \\(n\\) serial numbers. The maximum likelihood estimate for the total number of serial numbers is \\(X_{(n)}\\), the maximum of the observed serial numbers. However, this is a biased estimator – it underestimates the true number of tanks.\nWe can improve this estimator by adding a factor to our maximum likelihood estimator to make it unbiased – the number of known missing tank serial numbers divided by the sample size, which can be thought of as the average gap between the recorded serial numbers. Thus the new estimator is \\[\n\\hat{\\theta}=x_{(n)}+\\frac{x_{(n)}-n}{n} = \\left(1+\\frac{1}{n}\\right) x_{(n)}-1\n\\] which has the desired property of being unbiased. Simulating the performance of our estimators with a known population size of 2,000 tanks, we see that bias-corrected MLE has higher variance than the MLE but it achieves a lower mean squared error across the tested sample sizes due to being unbiased.\n\nThere are \\(X_{(n)} - n\\) known missing tank serial numbers.\n\n\nMy analysis follows that given in Leemis, L. (2020). Mathematical Statistics.\n\n\n\nVideo\n\n\n\n\nComparing Estimators for Unif(0, θ)\nThe maximum likelihood estimator (MLE), while it has many nice statistics properties, isn’t always the best estimator. To estimate θ for Unif(0,θ), the MLE is the maximum of the data, which is biased low. It is an underestimate of the true value of θ. To correct the bias, we can multiply the maximum by \\(\\frac{n+1}{n}\\) for an estimator that is unbiased, but has slightly higher variance. However, we see that the result is lower mean-squared error (MSE). We also show the method of moments estimator for reference, which is twice the sample mean.\n\n\nVideo\n\n\n\n\nComparing Estimators for Center of Unif(0,1)\nEstimators of central tendency can have very different properties. For Unif(0,1), the mean, median, and midrange (defined by \\(\\frac{\\text{max} - \\text{min}}{2}\\)) are consistent estimators of the center, but the median has high variance and the midrange is biased.\n\n\nVideo\n\n\n\n\nSampling Distribution of the Mean for the Gamma Distribution\nAs sample size increases, the sampling distribution of the mean\n\ndecreases in variance\napproaches a normal distribution (central limit theorem)\nhas expected value equal to population mean\n\nFor example, mean of N i.i.d samples from Gamma(a,b) is distributed Gamma(Na, Nb).\n\n\nVideo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Logistic regression (by hand)\n\n\n\n\n\nAn in-depth dive into the workings of logistic regression.\n\n\n\n\n\n\nMay 11, 2022\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nLinear Algebra / Matrix Operations in R\n\n\n\n\n\nInformal notes about common linear algebra / matrix operations in R\n\n\n\n\n\n\nApr 29, 2022\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nUsing the tmerge() function to structure time-dependent covariates for survival analysis\n\n\n\n\n\nThe tmerge() function in the survival package is used to structure data to represent time-dependent variables in a survival analysis. This post shows a minimal example of how to use tmerge.\n\n\n\n\n\n\nMar 31, 2022\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nSensitivity of odds-ratios calculated on dichotomized variables to inclusion criteria\n\n\n\n\n\nA short simulation example showing why dichomization of continuous variables can lead to wrong conclusions.\n\n\n\n\n\n\nJan 16, 2022\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nCreating a bookdown book with the bs4 theme\n\n\n\n\n\nA short guide to creating a bookdown book using the bs4 theme\n\n\n\n\n\n\nAug 10, 2021\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nWhat is Gradient Descent? (Part I)\n\n\n\n\n\nExploring gradient descent using R and a minimal amount of mathematics\n\n\n\n\n\n\nJan 16, 2021\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nStatistical simulation of robust estimators with tidyverse tools\n\n\n\n\n\nFunctions from the tidyverse provide a powerful way to do statistical simulations. We demonstrate this approach by evaluating the properties of the mean and median as estimators of center for two distributions.”\n\n\n\n\n\n\nJan 13, 2021\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nUsing gganimate to create cartographic animations\n\n\n\n\n\nThe gganimate package can be used with ggmap to create animations of geographic data. I show examples using data from Nashville Open Data.\n\n\n\n\n\n\nJan 6, 2021\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nRepeated testing inflates type I error\n\n\n\n\n\nType I error is increased when you test your hypothesis multiple times during the data collection process. Simulations can provide a clear picture of this process.\n\n\n\n\n\n\nDec 23, 2020\n\n\nMax Rohde\n\n\n\n\n\n\n  \n\n\n\n\nWhat is a statistic?\n\n\n\n\n\nExploring the idea of a statistic by simulating dice rolls in R\n\n\n\n\n\n\nDec 8, 2020\n\n\nMax Rohde\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bs4/index.html",
    "href": "posts/bs4/index.html",
    "title": "Creating a bookdown book with the bs4 theme",
    "section": "",
    "text": "If you’ve been learning R through the amazing free resources made available by the R community, then you probably seen examples of the bs4 bookdown theme in a few of these various sources:\n\nR for Data Science: https://r4ds.had.co.nz\n\nThe ggplot2 book: https://ggplot2-book.org\n\nMastering Shiny: https://mastering-shiny.org\n\n\nI think the bs4 theme looks better, and is easier to navigate, than the older gitbook theme (example here).\nThis is a quick guide on how to get started. For comprehensive details, look at the official guide by Yihui Xie, the author of bookdown."
  },
  {
    "objectID": "posts/bs4/index.html#how-do-you-use-the-bs4-theme",
    "href": "posts/bs4/index.html#how-do-you-use-the-bs4-theme",
    "title": "Creating a bookdown book with the bs4 theme",
    "section": "How do you use the bs4 theme?",
    "text": "How do you use the bs4 theme?\nTo follow this tutorial, you need to have the development version of bookdown installed. You can install it with\n\nCoderemotes::install_github('rstudio/bookdown')\n\n\nIt will also be helpful to work in RStudio, although it is not strictly necessary.\nWe will create an example project using the bs4 theme, using a template provided by the developers. Then we can edit this template project to make our bookdown book. To create an example project, enter the following command into the R console within RStudio:\n\nCodebookdown::create_bs4_book(\"your_project_name_here\")\n\n\nThis will create the project in your current directory.\nNow, click on the .Rproj file within the project directory to open the project in RStudio. You will see the files of the example like this:\n\n\nExamples bs4 files\n\n\nYou can then edit the files according to your needs. The first file to start editing is index.rmd, which contains the metadata of the book such as the title, author, date and more. It also contains the content that will go on the landing page for the HTML rendering of the book.\nNext you will want to edit files for each chapter, and delete those you don’t need.\nLastly, to fiddle with the theme, you can edit the style.css file if you have some experience with CSS.\nTo render the book, use\n\nCodebookdown::render_book()\n\n\nThen to view the preview of the HTML page, use\n\nCodebookdown::serve_book()\n\n\n\n\nThe final result\n\n\nTo host my bookdown books online, I put the project into a GitHub repository, and then use Netlify to host it for free. However, this is out of the scope of this tutorial. The official guide by Yihui Xie has more details on the hosting process."
  },
  {
    "objectID": "posts/early-stopping/index.html",
    "href": "posts/early-stopping/index.html",
    "title": "Repeated testing inflates type I error",
    "section": "",
    "text": "Does your astrological sign affect your height? Imagine that 20 scientists want to test this hypothesis. Each scientist, independently of the others, brings in 1,000 pairs of twins – one Taurus and one Leo – and measures the difference in their heights. Assume that in reality, the null hypothesis is true: there is no difference in their heights. Of course, there is random variation that will make each twin’s height different, but the null hypothesis is that the mean of this variation is zero.\n\nEspecially for height, it’s safe to assume that the variation in heights is normally distributed.\n\nEach scientists measures their 1,000 pairs of twins, conducts a t-test, and reports whether or not their statistical test has rejected the null hypothesis of no difference. If the null hypothesis is true (which we assume it is in this scenario), and the p-value threshold for significance is set at 0.05 = 1/20, we would expect that on average, 1 out of the 20 scientists would wrongly reject the null hypothesis when it is in fact true. This is the definition of type I error rate. But what if the scientists took multiple looks at the data?\nIt may have began as a way to save money – participants can be expensive! Starting with the 10th pair of twins, each scientist tests their hypothesis with a statistical test after every data point comes in. Measure the 11th pair. Run the test on the 11 data points. Measure the 12th pair. Run the test on the 12 data points. And so on. If any of these tests are significant, can the scientist claim that their result is statistically significant at the 0.05 level? Is the type I error rate of their testing procedure controlled at 0.05?\nThe answer is wholeheartedly, no. Repeated testing of the data will greatly inflate the type I error rate. Instead of the nominal 1/20 probability to wrongly reject the null hypothesis when it is true, the type I error rate of this sequential testing procedure will be much higher. If sampling continues indefinitely, sequential testing is guaranteed to reach a significant result. Edwards, Lindman, and Savage (1963) phrase this succinctly: “And indeed if an experimenter uses this procedure, then with probability 1 he will eventually reject any sharp null hypothesis, even though it be true.”\n\nEdwards, W., Lindman, H., & Savage, L. J. (1963). Bayesian statistical inference for psychological research. Psychological review, 70(3), 193."
  },
  {
    "objectID": "posts/early-stopping/index.html#simulation-using-r",
    "href": "posts/early-stopping/index.html#simulation-using-r",
    "title": "Repeated testing inflates type I error",
    "section": "Simulation using R",
    "text": "Simulation using R\nUsing R, let’s simulate the scenario described above and see for ourselves what happens to the type I error rate. To recap, we have 20 independent scientists conducting their own experiment. After every data point comes in, they conduct a statistical testing using the data they have collected so far. As we can see by the wiggling lines, given that the null hypothesis is true, the p-value fluctuate wildly with each new test. If a scientist obtains a p-value < 0.05 in the course of the experiment, the line past that observation is colored red so that we can identify when significance has been declared by each scientist.\n\nThe animations were created using the gganimate package.\n\n\n\nVideo\n\n\nOn the right panel, the cumulative type I error rate and the type I error rate at each observation is shown. We can see that by the time all 1000 data points are recorded, close to half of the scientists have declared significance at some point in the process! This error rate is clearly higher than 1/20 as would be expected without sequential testing. At any given time point however, the type I error rate is preserved: only about 1 scientist will have wrongly declared significance.\nTo verify our conclusions, let’s run this simulation for 2,000 scientists instead of 20 and observe the type I error rate.\n\n\nVideo\n\n\nAs seen before, the cumulative type I error rate increases the more looks the scientists take at the data, reaching almost 50% by 1,000 observations. Contrast this to type I error rate at any given observation, which remains controlled at the nominal value of 0.05."
  },
  {
    "objectID": "posts/early-stopping/index.html#so-how-can-we-control-type-i-error",
    "href": "posts/early-stopping/index.html#so-how-can-we-control-type-i-error",
    "title": "Repeated testing inflates type I error",
    "section": "So how can we control type I error?",
    "text": "So how can we control type I error?\nHow can we fix this problem? One approach is creating rules limiting the number of interim tests that can be conducted. For example, a topic of recent relevance, there was much debate over how many interim looks at the data there should be for the COVID-19 vaccine trials.\nAnother approach is applying corrections to the p-value threshold to ensure that the overall type I error rate is still 5%. The field of sequential analysis is concerned with these types of problems, and the solutions can be mathematically complex."
  },
  {
    "objectID": "posts/gganimate-map/index.html",
    "href": "posts/gganimate-map/index.html",
    "title": "Using gganimate to create cartographic animations",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(ggmap)\nlibrary(gganimate)\n\n# ggplot2 themes\nlibrary(cowplot)\n\n# Formatting of HTML tables\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/gganimate-map/index.html#overview",
    "href": "posts/gganimate-map/index.html#overview",
    "title": "Using gganimate to create cartographic animations",
    "section": "Overview",
    "text": "Overview\nAnimating your ggplot2 visualizations is easy using the gganimate package. But did you also know that gganimate can be used with the ggmap package to animate geographic data? Using data from Nashville Open Data, we’ll create an animation to visualize the development of parks in Nashville over time."
  },
  {
    "objectID": "posts/gganimate-map/index.html#data-cleaning-and-exploration",
    "href": "posts/gganimate-map/index.html#data-cleaning-and-exploration",
    "title": "Using gganimate to create cartographic animations",
    "section": "Data cleaning and exploration",
    "text": "Data cleaning and exploration\nFirst, we load in the data.\n\nCode# Read in data\ndf <-\n  read_csv(\"Parks_Property_2016.csv\") %>%\n  janitor::clean_names() %>%\n  select(lat, long = lon, year_established = year_estab, acres, park_name =  common_nam)\n\n\n\nCodedf %>% \n  kable(format = \"html\") %>%\n  kable_styling(\"striped\") %>%\n  scroll_box(height=\"500px\")\n\n\n\n\n lat \n    long \n    year_established \n    acres \n    park_name \n  \n\n\n 3.607738e+01 \n    -86.95922 \n    1988 \n    69.8600000 \n    Harpeth River \n  \n\n 3.605943e+01 \n    -86.93816 \n    1972 \n    9.8800000 \n    Harpeth Knoll \n  \n\n 3.607160e+01 \n    -86.93354 \n    1982 \n    17.1400000 \n    Red Caboose \n  \n\n 3.615017e+01 \n    -86.92646 \n    2007 \n    809.5900000 \n    Bells Bend \n  \n\n 3.626076e+01 \n    -86.91984 \n    1996 \n    2168.7920000 \n    Beaman \n  \n\n 3.605688e+01 \n    -86.90744 \n    1937 \n    1113.1200000 \n    Edwin Warner \n  \n\n 3.613620e+01 \n    -86.88757 \n    2012 \n    8.5300000 \n    H. G. Hill \n  \n\n 3.606729e+01 \n    -86.88420 \n    1926 \n    1991.5600000 \n    Percy Warner \n  \n\n 3.615454e+01 \n    -86.87071 \n    1964 \n    25.2400000 \n    Charlotte \n  \n\n 3.631718e+01 \n    -86.87022 \n    1989 \n    18.3800000 \n    Joelton \n  \n\n 3.616150e+01 \n    -86.86422 \n    1952 \n    33.4500000 \n    West \n  \n\n 3.617850e+01 \n    -86.83915 \n    1964 \n    12.2600000 \n    Bordeaux Garden \n  \n\n 3.611866e+01 \n    -86.83532 \n    1988 \n    3.2400000 \n    Woodmont \n  \n\n 3.615925e+01 \n    -86.83257 \n    1947 \n    11.2800000 \n    Boyd -Taylor \n  \n\n 3.634344e+01 \n    -86.86116 \n    2013 \n    98.4100000 \n    Paradise Ridge \n  \n\n 3.610406e+01 \n    -86.86017 \n    1988 \n    8.3900000 \n    Parmer \n  \n\n 3.620875e+01 \n    -86.82747 \n    1964 \n    54.2000000 \n    Hartman \n  \n\n 3.613357e+01 \n    -86.82402 \n    1927 \n    13.0700000 \n    Elmington \n  \n\n 3.615638e+01 \n    -86.85505 \n    2014 \n    11.2500000 \n    England \n  \n\n 3.616553e+01 \n    -86.82243 \n    1912 \n    30.7000000 \n    Hadley \n  \n\n 3.619058e+01 \n    -86.82113 \n    1954 \n    261.1700000 \n    Rhodes \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.3400000 \n    East Jr. Tennis Courts \n  \n\n 3.620718e+01 \n    -86.85206 \n    2012 \n    34.8500000 \n    Mullins \n  \n\n 3.613922e+01 \n    -86.84705 \n    1938 \n    208.9800000 \n    McCabe \n  \n\n 3.615153e+01 \n    -86.84464 \n    1907 \n    9.9800000 \n    Richland \n  \n\n 3.621440e+01 \n    -86.84351 \n    1980 \n    16.8100000 \n    Bordeaux-Timothy \n  \n\n 3.615160e+01 \n    -86.79200 \n    1975 \n    0.5900000 \n    Owen Bradley \n  \n\n 3.627492e+01 \n    -86.81879 \n    1978 \n    19.7000000 \n    Whites Creek \n  \n\n 3.615774e+01 \n    -86.81796 \n    1969 \n    1.5000000 \n    McKissack \n  \n\n 3.617482e+01 \n    -86.81664 \n    1983 \n    7.3600000 \n    Fisk \n  \n\n 3.611058e+01 \n    -86.80866 \n    1999 \n    1.5300000 \n    Sally Beaman \n  \n\n 3.613682e+01 \n    -86.80631 \n    1978 \n    7.6500000 \n    Dragon \n  \n\n 3.609537e+01 \n    -86.80484 \n    1973 \n    13.0300000 \n    Green Hills \n  \n\n 3.617866e+01 \n    -86.80315 \n    1909 \n    1.6000000 \n    Elizabeth \n  \n\n 3.613167e+01 \n    -86.80244 \n    1996 \n    1.7100000 \n    St. Bernard \n  \n\n 3.616257e+01 \n    -86.79951 \n    1901 \n    8.1200000 \n    Watkins \n  \n\n 3.615941e+01 \n    -86.79734 \n    1979 \n    0.8600000 \n    Edmondson \n  \n\n 3.614940e+01 \n    -86.81340 \n    1903 \n    125.1400000 \n    Centennial \n  \n\n 3.617086e+01 \n    -86.79474 \n    2000 \n    0.4000000 \n    Hope Gardens \n  \n\n 3.618075e+01 \n    -86.78999 \n    1910 \n    6.6300000 \n    Morgan \n  \n\n 3.614644e+01 \n    -86.78956 \n    2012 \n    0.3500000 \n    Flora Wilson \n  \n\n 3.615004e+01 \n    -86.78945 \n    1975 \n    2.3100000 \n    Tony Rose \n  \n\n 3.611917e+01 \n    -86.78919 \n    1945 \n    20.4100000 \n    Sevier \n  \n\n 3.619941e+01 \n    -86.78749 \n    1969 \n    3.4200000 \n    Lock One \n  \n\n 3.604615e+01 \n    -86.75473 \n    1979 \n    6.9500000 \n    Granbery \n  \n\n 3.614350e+01 \n    -86.78345 \n    1960 \n    25.0800000 \n    Rose \n  \n\n 3.616269e+01 \n    -86.78181 \n    2001 \n    0.2800000 \n    Church Street \n  \n\n 3.613750e+01 \n    -86.78087 \n    1914 \n    15.9000000 \n    Reservoir \n  \n\n 3.613438e+01 \n    -86.77809 \n    1981 \n    1.3200000 \n    Neil \n  \n\n 3.616685e+01 \n    -86.77809 \n    2006 \n    6.0100000 \n    Public Square \n  \n\n 3.615921e+01 \n    -86.77682 \n    2005 \n    2.6700000 \n    Walk of Fame \n  \n\n 3.616832e+01 \n    -86.75391 \n    1963 \n    7.9600000 \n    Kirkpatrick \n  \n\n 3.621581e+01 \n    -86.75174 \n    1983 \n    26.2800000 \n    Oakwood \n  \n\n 3.617000e+01 \n    -86.78000 \n    1977 \n    3.4900000 \n    Bicentennial \n  \n\n 3.616222e+01 \n    -86.77611 \n    2000 \n    0.3100000 \n    Commerce Center \n  \n\n 3.616424e+01 \n    -86.77530 \n    1930 \n    0.6600000 \n    Fort Nashborough \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Crieve Hall School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Jordonia School Playground \n  \n\n 3.614503e+01 \n    -86.77421 \n    1928 \n    57.3900000 \n    Fort Negley \n  \n\n 3.616256e+01 \n    -86.77404 \n    1983 \n    5.2900000 \n    Riverfront \n  \n\n 3.623509e+01 \n    -86.77146 \n    1973 \n    10.4600000 \n    Parkwood \n  \n\n 3.625341e+01 \n    -86.77143 \n    2003 \n    12.1300000 \n    Willow Creek \n  \n\n 3.607724e+01 \n    -86.72783 \n    2007 \n    13.5300000 \n    Caldwell \n  \n\n 3.616249e+01 \n    -86.77058 \n    2012 \n    4.7551980 \n    Cumberland \n  \n\n 3.618177e+01 \n    -86.76702 \n    1920 \n    11.1300000 \n    McFerrin \n  \n\n 3.614385e+01 \n    -86.76534 \n    1913 \n    6.4400000 \n    Dudley \n  \n\n 3.627176e+01 \n    -86.74802 \n    1965 \n    321.4600000 \n    Cedar Hill \n  \n\n 3.617057e+01 \n    -86.74634 \n    1996 \n    0.5600000 \n    Shelby Walk \n  \n\n 3.610819e+01 \n    -86.74541 \n    2006 \n    0.7430000 \n    Turner School \n  \n\n 3.617793e+01 \n    -86.76230 \n    1963 \n    31.9400000 \n    Douglas \n  \n\n 3.617287e+01 \n    -86.76041 \n    1916 \n    10.6500000 \n    East Park \n  \n\n 3.618830e+01 \n    -86.75971 \n    1963 \n    18.4500000 \n    Cleveland \n  \n\n 3.620367e+01 \n    -86.75968 \n    1991 \n    6.8100000 \n    Tom Joy \n  \n\n 3.615160e+01 \n    -86.75898 \n    1983 \n    0.4000000 \n    Mildred Shute \n  \n\n 3.615007e+01 \n    -86.75654 \n    1913 \n    2.2200000 \n    Napier \n  \n\n 3.611188e+01 \n    -86.74425 \n    1939 \n    8.7700000 \n    Coleman \n  \n\n 3.617516e+01 \n    -86.74182 \n    1921 \n    0.1800000 \n    Bass \n  \n\n 3.608775e+01 \n    -86.74002 \n    0 \n    196.5900000 \n    Grassmere \n  \n\n 3.606692e+01 \n    -86.73968 \n    1978 \n    13.6700000 \n    Whitfield \n  \n\n 3.619271e+01 \n    -86.73947 \n    1953 \n    2.4400000 \n    Eastland \n  \n\n 3.617616e+01 \n    -86.73717 \n    2013 \n    2.2300000 \n    Lockeland Springs \n  \n\n 3.617055e+01 \n    -86.73295 \n    1912 \n    343.3100000 \n    Shelby \n  \n\n 3.618291e+01 \n    -86.70550 \n    1997 \n    950.1800000 \n    Shelby Bottoms \n  \n\n 3.619700e+01 \n    -86.72762 \n    1972 \n    19.1400000 \n    South Inglewood \n  \n\n 3.622800e+01 \n    -86.72445 \n    1995 \n    5.7000000 \n    Litton \n  \n\n 3.609511e+01 \n    -86.71480 \n    1973 \n    35.6800000 \n    Paragon Mills \n  \n\n 3.609215e+01 \n    -86.70763 \n    2003 \n    4.1900000 \n    Providence \n  \n\n 3.605723e+01 \n    -86.70645 \n    1982 \n    26.9400000 \n    Pitts \n  \n\n 3.624415e+01 \n    -86.70246 \n    1956 \n    5.7300000 \n    Lock Two \n  \n\n 3.626582e+01 \n    -86.70099 \n    1988 \n    30.7400000 \n    Madison \n  \n\n 3.610830e+01 \n    -86.69954 \n    1963 \n    76.0100000 \n    Seven Oaks \n  \n\n 3.618991e+01 \n    -86.68239 \n    1966 \n    374.3000000 \n    Two Rivers \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Andrew Jackson School Playground \n  \n\n 3.608808e+01 \n    -86.67803 \n    1978 \n    79.9400000 \n    Ezell \n  \n\n 3.605559e+01 \n    -86.67340 \n    1950 \n    7.9800000 \n    Antioch \n  \n\n 3.618528e+01 \n    -86.66682 \n    2002 \n    74.3800000 \n    Heartland \n  \n\n 3.620519e+01 \n    -86.65257 \n    1965 \n    649.7100000 \n    Peeler \n  \n\n 3.609890e+01 \n    -86.63865 \n    2007 \n    24.9800000 \n    Una \n  \n\n 3.600102e+01 \n    -86.63764 \n    1971 \n    6.2500000 \n    Crawford \n  \n\n 3.624013e+01 \n    -86.63373 \n    2011 \n    4.4000000 \n    Lakewood \n  \n\n 3.618753e+01 \n    -86.63272 \n    2006 \n    9.7600000 \n    Stone Hall \n  \n\n 3.610490e+01 \n    -86.62597 \n    1980 \n    381.3600000 \n    Hamilton Creek \n  \n\n 3.599103e+01 \n    -86.61878 \n    1983 \n    274.1300000 \n    Cane Ridge \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Kings Lane School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Dan Mills School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Pennington Bend School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Schwab School Playground \n  \n\n 3.617756e+01 \n    -86.61507 \n    1999 \n    22.1900000 \n    Hermitage \n  \n\n 3.617318e+01 \n    -86.79860 \n    1984 \n    0.6800000 \n    Monroe \n  \n\n 3.618239e+01 \n    -86.85325 \n    1963 \n    19.7800000 \n    City Cemetery \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Tulip Grove School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Hermitage School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Hattie Cotton School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Rosebank School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Stanford School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Hickman Elm School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Cockrill School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Park Ave School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Jones Paideia School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    1.0200000 \n    Hillwood School Tennis Courts \n  \n\n 3.610288e+01 \n    -86.68115 \n    2006 \n    64.1700000 \n    Soccer Complex \n  \n\n 3.618239e+01 \n    -86.85325 \n    1976 \n    5.0000000 \n    County Cemetery \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Glencliff School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.6800000 \n    Glencliff Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Lakeview School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Norman Binkley School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Haywood School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.5100000 \n    Bellevue Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Moss School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.5100000 \n    Ewing Park Tennis Courts \n  \n\n 3.619124e+01 \n    -86.81393 \n    1934 \n    46.4800000 \n    Buena Vista \n  \n\n 3.619149e+01 \n    -86.81267 \n    1934 \n    4.9700000 \n    Potters Field \n  \n\n 3.613298e+01 \n    86.90497 \n    2003 \n    14.1400000 \n    Brookmeade \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Stratton School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Dupont School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Neely's Bend School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.5100000 \n    Neely's Bend Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.5100000 \n    Maplewood Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Percy Priest School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Tusculum School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.5100000 \n    Mcmurray Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Cole School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Mt. View School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Charlotte School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Old Center School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Paragon Mills School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Whitsett School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Chadwell School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.6800000 \n    Hillsboro Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Tom Joy School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Una School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Sylvan Park School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.6800000 \n    Overton Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Amquie School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Harpeth School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Julia Green School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Alex Green School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Mcgavock School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Ruby Major School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Cora Howe School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Shayne School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Dodson School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Maxwell School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.5100000 \n    Stratford Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Fall-Hamilton School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Edmonson Library Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Hull Jackson School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Ross School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.5100000 \n    Granbery School Tennis Courts \n  \n\n 0.000000e+00 \n    0.00000 \n    0 \n    0.3400000 \n    Antioch Tennis Courts \n  \n\n 3.624002e+01 \n    -86.64811 \n    2012 \n    84.0000000 \n    Crooked Branch \n  \n\n 3.611668e+01 \n    -86.78903 \n    2013 \n    5.4000000 \n    Gale Lane \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.1345503 \n    Cumberland River Greenway - Metro Center \n  \n\n 3.625271e+01 \n    -86.64679 \n    2013 \n    6.0100000 \n    Old Hickory Baseball \n  \n\n 3.625579e+01 \n    -86.64605 \n    2013 \n    2.4100000 \n    Rachel's Walk \n  \n\n 3.626437e+01 \n    -86.65058 \n    2013 \n    0.4500000 \n    Veterans \n  \n\n 3.626542e+01 \n    -86.64965 \n    2013 \n    13.5800000 \n    Old Hickory \n  \n\n 3.626393e+01 \n    -86.65150 \n    2013 \n    0.1200000 \n    Old Hickory Community Center \n  \n\n 3.625664e+01 \n    -86.65259 \n    2013 \n    1.6100000 \n    Old Hickory Arts \n  \n\n 3.625845e+01 \n    -86.64896 \n    2013 \n    2.3900000 \n    12th Street Ballfield \n  \n\n 3.611722e+01 \n    -86.71963 \n    2013 \n    62.6100000 \n    Whitsett \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    1.5553245 \n    Stones River Greeenway - Two Rivers to Lytle \n  \n\n 3.601296e+01 \n    -86.69024 \n    2013 \n    85.6200000 \n    Mill Creek \n  \n\n 3.618238e+01 \n    -86.65910 \n    2013 \n    615.9000000 \n    Lytle Farm \n  \n\n 3.616015e+01 \n    -86.77186 \n    2015 \n    12.5400000 \n    Ascend Amphitheater \n  \n\n 3.605297e+01 \n    -86.65626 \n    2014 \n    12.1363239 \n    Southeast Community Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    0.4646216 \n    Stones River Greeenway - Two Rivers to Lytle \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    2.3579139 \n    Stones River Greeenway - Two Rivers to Lytle \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    0.1121120 \n    Stones River Greeenway - Two Rivers to Lytle \n  \n\n 1.735803e+06 \n    666357.14396 \n    2012 \n    0.6655799 \n    Gulch Greenway \n  \n\n 1.736155e+06 \n    665443.67692 \n    2015 \n    0.4948844 \n    Gulch Greenway \n  \n\n 1.737172e+06 \n    670336.24990 \n    2001 \n    0.2594513 \n    Cumberland River Greenway - Bicentennial Connector \n  \n\n 3.619003e+01 \n    -86.64606 \n    2015 \n    172.4690974 \n    Ravenwood \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    2.5000000 \n    Cumberland River Greenway - Bicentennial Connector \n  \n\n 3.611345e+01 \n    -86.77666 \n    2012 \n    0.3481739 \n    Browns Creek Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    0.7298034 \n    Cumberland River Greenway - SB trailhead \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.0493584 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.0818948 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.0117001 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    1.7948271 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.1613896 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.0167640 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.2653782 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    11.9566555 \n    Whites Creek Greenway - Hartman Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2005 \n    2.7587219 \n    Whites Creek Greenway - Hartman Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.8719442 \n    Whites Creek Greenway - Hartman Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    1.0000000 \n    Whites Creek Greenway - Hartman Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.5361007 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.6990988 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    2.1957676 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.6213730 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    1.0171761 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.8803449 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    1.4035318 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.1757014 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.1910113 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.3268539 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.8981220 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.1643947 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.8378213 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    2.2856454 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.1554352 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.4177268 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.6409715 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    0.2436326 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.6941219 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.1009122 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.1245130 \n    Cumberland River Greenway - Metro Center \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    0.2207411 \n    Cumberland River Greenway - Bicentennial Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    0.4808856 \n    Cumberland River Greenway - Bicentennial Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    1.0315835 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    0.0586311 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    2009 \n    1.8924259 \n    Cumberland River Greenway - Boyd Taylor \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    1.1242000 \n    Cumberland River Greenway - Downtown \n  \n\n 0.000000e+00 \n    0.00000 \n    1995 \n    4.7682243 \n    Harpeth River Greenway - Warner - Morton Mill \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    23.1109609 \n    Harpeth River Greenway - Warner - Warner Exchange \n  \n\n 0.000000e+00 \n    0.00000 \n    2013 \n    33.2810709 \n    Harpeth River Greenway - Warner - Harpeth Bend \n  \n\n 0.000000e+00 \n    0.00000 \n    2013 \n    3.1920090 \n    Harpeth River Greenway - Warner - Harpeth Valley \n  \n\n 0.000000e+00 \n    0.00000 \n    1995 \n    9.4195271 \n    Harpeth River Greenway - Warner - Morton Mill \n  \n\n 0.000000e+00 \n    0.00000 \n    2013 \n    9.0121617 \n    Harpeth River Greenway - Warner - Harpeth Crest \n  \n\n 0.000000e+00 \n    0.00000 \n    2013 \n    4.0313277 \n    Harpeth River Greenway - Harpeth Springs \n  \n\n 0.000000e+00 \n    0.00000 \n    2013 \n    12.0255760 \n    Harpeth River Greenway - Harpeth Springs \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    7.8561414 \n    Harpeth River Greenway - River Walk \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    1.4861100 \n    Harpeth River Greenway - River Walk \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    1.8113980 \n    Harpeth River Greenway - River Walk \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    7.1464777 \n    Harpeth River Greenway - River Walk \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    10.9477867 \n    Harpeth River Greenway - River Walk \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    5.5703800 \n    Harpeth River Greenway - River Walk \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    2.4222613 \n    Harpeth River Greenway - River Walk \n  \n\n 0.000000e+00 \n    0.00000 \n    2013 \n    0.1684959 \n    Harpeth River Greenway - Warner - Harpeth Bend \n  \n\n 0.000000e+00 \n    0.00000 \n    1995 \n    2.2458589 \n    Harpeth River Greenway - Warner - Morton Mill \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    32.7153412 \n    Mill Creek Greenway - Blue Hole \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    6.8735880 \n    Mill Creek Greenway - Blue Hole \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    5.0335253 \n    Mill Creek Greenway - Ezell Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2001 \n    3.5095535 \n    Mill Creek Greenway - Ezell Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2014 \n    3.5486258 \n    Mill Creek Greenway - Mill Creek Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    50.2175012 \n    Mill Creek Greenway - Blue Hole \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    10.5583895 \n    Mill Creek Greenway - Blue Hole \n  \n\n 0.000000e+00 \n    0.00000 \n    2015 \n    25.9882102 \n    Mill Creek Greenway-Rivendell \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    2.2191096 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2014 \n    9.6707414 \n    Mill Creek Greenway - Mill Creek Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2014 \n    0.3969118 \n    Mill Creek Greenway - Mill Creek Park \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    9.5489080 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    3.9594254 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    1.1946428 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2006 \n    1.3220493 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    0.0532986 \n    Richland Creek Creenway - Lion's Head Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2004 \n    15.3544355 \n    Richland Creek Creenway -McCabe Loop \n  \n\n 0.000000e+00 \n    0.00000 \n    2010 \n    2.0813361 \n    Richland Creek Creenway - Knob Road Connector \n  \n\n 0.000000e+00 \n    0.00000 \n    2013 \n    1.7147314 \n    Cumberland River Greenway - Rolling Mill Hill \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    3.5618533 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    53.2851401 \n    Seven Mile Creek Greenway - Ellington Ag \n  \n\n 0.000000e+00 \n    0.00000 \n    2008 \n    1.5429462 \n    Seven Mile Creek Greenway - Harding Mall \n  \n\n 0.000000e+00 \n    0.00000 \n    2013 \n    1.1000000 \n    Cumberland River Greenway - Rolling Mill Hill \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    0.1000000 \n    Smithson School Playground \n  \n\n 0.000000e+00 \n    0.00000 \n    2000 \n    16.8742281 \n    Stones River Greenway - Alta Lake \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    5.0076261 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    0.6513379 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0.000000e+00 \n    0.00000 \n    2005 \n    16.6344681 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0.000000e+00 \n    0.00000 \n    2014 \n    135.4692117 \n    Whites Creek Greenway - Fontanel \n  \n\n 0.000000e+00 \n    0.00000 \n    1999 \n    19.2924154 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    6.4750941 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    3.7454341 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0.000000e+00 \n    0.00000 \n    2000 \n    1.4684997 \n    Stones River Greenway - Alta Lake \n  \n\n 0.000000e+00 \n    0.00000 \n    2003 \n    0.1818802 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0.000000e+00 \n    0.00000 \n    2014 \n    7.3042493 \n    Whites Creek Community Garden \n  \n\n 0.000000e+00 \n    0.00000 \n    2002 \n    6.5039280 \n    Mill Creek Greenway - Whittimore Branch \n  \n\n\n\n\n\nBefore we create the animations, let’s do some data cleaning.\nAre parks duplicated?\n\nCodedf %>%\n  filter(duplicated(park_name)) %>%\n  kable(format = \"html\") %>%\n  kable_styling(\"striped\") %>%\n  scroll_box(height=\"300px\")\n\n\n\n\n lat \n    long \n    year_established \n    acres \n    park_name \n  \n\n\n 0 \n    0.0 \n    2003 \n    0.4646216 \n    Stones River Greeenway - Two Rivers to Lytle \n  \n\n 0 \n    0.0 \n    2003 \n    2.3579139 \n    Stones River Greeenway - Two Rivers to Lytle \n  \n\n 0 \n    0.0 \n    2003 \n    0.1121120 \n    Stones River Greeenway - Two Rivers to Lytle \n  \n\n 1736155 \n    665443.7 \n    2015 \n    0.4948844 \n    Gulch Greenway \n  \n\n 0 \n    0.0 \n    2001 \n    2.5000000 \n    Cumberland River Greenway - Bicentennial Connector \n  \n\n 0 \n    0.0 \n    2008 \n    0.0818948 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2008 \n    0.0117001 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2008 \n    1.7948271 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2008 \n    0.1613896 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2008 \n    0.0167640 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2008 \n    0.2653782 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2005 \n    2.7587219 \n    Whites Creek Greenway - Hartman Park \n  \n\n 0 \n    0.0 \n    2004 \n    0.8719442 \n    Whites Creek Greenway - Hartman Park \n  \n\n 0 \n    0.0 \n    2006 \n    1.0000000 \n    Whites Creek Greenway - Hartman Park \n  \n\n 0 \n    0.0 \n    2008 \n    0.5361007 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2008 \n    0.6990988 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2004 \n    2.1957676 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2008 \n    0.6213730 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2004 \n    1.0171761 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.8803449 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    1.4035318 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.1757014 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.1910113 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.3268539 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.8981220 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.1643947 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.8378213 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    2.2856454 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.1554352 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.4177268 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2008 \n    0.6409715 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2008 \n    0.2436326 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2004 \n    0.6941219 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.1009122 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2004 \n    0.1245130 \n    Cumberland River Greenway - Metro Center \n  \n\n 0 \n    0.0 \n    2001 \n    0.2207411 \n    Cumberland River Greenway - Bicentennial Connector \n  \n\n 0 \n    0.0 \n    2001 \n    0.4808856 \n    Cumberland River Greenway - Bicentennial Connector \n  \n\n 0 \n    0.0 \n    2001 \n    1.0315835 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2001 \n    0.0586311 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    2001 \n    1.1242000 \n    Cumberland River Greenway - Downtown \n  \n\n 0 \n    0.0 \n    1995 \n    9.4195271 \n    Harpeth River Greenway - Warner - Morton Mill \n  \n\n 0 \n    0.0 \n    2013 \n    12.0255760 \n    Harpeth River Greenway - Harpeth Springs \n  \n\n 0 \n    0.0 \n    2006 \n    1.4861100 \n    Harpeth River Greenway - River Walk \n  \n\n 0 \n    0.0 \n    2006 \n    1.8113980 \n    Harpeth River Greenway - River Walk \n  \n\n 0 \n    0.0 \n    2006 \n    7.1464777 \n    Harpeth River Greenway - River Walk \n  \n\n 0 \n    0.0 \n    2006 \n    10.9477867 \n    Harpeth River Greenway - River Walk \n  \n\n 0 \n    0.0 \n    2006 \n    5.5703800 \n    Harpeth River Greenway - River Walk \n  \n\n 0 \n    0.0 \n    2006 \n    2.4222613 \n    Harpeth River Greenway - River Walk \n  \n\n 0 \n    0.0 \n    2013 \n    0.1684959 \n    Harpeth River Greenway - Warner - Harpeth Bend \n  \n\n 0 \n    0.0 \n    1995 \n    2.2458589 \n    Harpeth River Greenway - Warner - Morton Mill \n  \n\n 0 \n    0.0 \n    2002 \n    6.8735880 \n    Mill Creek Greenway - Blue Hole \n  \n\n 0 \n    0.0 \n    2001 \n    3.5095535 \n    Mill Creek Greenway - Ezell Park \n  \n\n 0 \n    0.0 \n    2002 \n    50.2175012 \n    Mill Creek Greenway - Blue Hole \n  \n\n 0 \n    0.0 \n    2002 \n    10.5583895 \n    Mill Creek Greenway - Blue Hole \n  \n\n 0 \n    0.0 \n    2014 \n    9.6707414 \n    Mill Creek Greenway - Mill Creek Park \n  \n\n 0 \n    0.0 \n    2014 \n    0.3969118 \n    Mill Creek Greenway - Mill Creek Park \n  \n\n 0 \n    0.0 \n    2006 \n    9.5489080 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0 \n    0.0 \n    2006 \n    3.9594254 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0 \n    0.0 \n    2006 \n    1.1946428 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0 \n    0.0 \n    2006 \n    1.3220493 \n    Richland Creek Creenway - White Bridge Connector \n  \n\n 0 \n    0.0 \n    2013 \n    1.1000000 \n    Cumberland River Greenway - Rolling Mill Hill \n  \n\n 0 \n    0.0 \n    2003 \n    5.0076261 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0 \n    0.0 \n    2003 \n    0.6513379 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0 \n    0.0 \n    2005 \n    16.6344681 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0 \n    0.0 \n    1999 \n    19.2924154 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0 \n    0.0 \n    2003 \n    6.4750941 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0 \n    0.0 \n    2003 \n    3.7454341 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n 0 \n    0.0 \n    2000 \n    1.4684997 \n    Stones River Greenway - Alta Lake \n  \n\n 0 \n    0.0 \n    2003 \n    0.1818802 \n    Stones River Greenway - Lebanon Pk to Dam \n  \n\n\n\n\n\nThese rows look like errors so we will remove them.\n\nCodedf <-\ndf %>%\n  filter(!(lat == 0),\n         !(long == 0)) %>%\n  distinct(park_name, .keep_all = TRUE)\n\n\nWe also need to remove unreasonable lat/long values.\n\nCodedf <-\ndf %>%\n  filter(!(abs(lat) >= 180),\n         !(abs(long) >= 180))\n\n\n\nCode# Convert all long values to negative because of an error\ndf$long <- abs(df$long) * -1\n\n\nHow many parks are there?\n\nCode(num_parks <-nrow(df))\n\n[1] 123\n\n\n123 parks in Nashville – not bad!\nWhich years are represented in the data?\n\nCoderange(df$year_established)\n\n[1]    0 2015\n\n\nStrange, it looks like the oldest park was established in year “0”. This must be a mistake.\n\nCode# Newest 5 parks\nsort(df$year_established) %>% tail()\n\n[1] 2013 2013 2014 2014 2015 2015\n\nCode# Oldest 5 parks\nsort(df$year_established) %>% head()\n\n[1]    0 1901 1903 1907 1909 1910\n\n\nThe true range of the data is 1901 - 2015. It looks like the park recorded as being established in year zero was a mistake. Let’s remove it from the dataset.\n\nCodedf <-\n  df %>%\n  filter(year_established != 0)\n\n\nNow let’s take a look at the distribution of the years when parks were established. First, we’ll make a histogram.\n\nCodedf %>%\n  ggplot() +\n  aes(year_established) +\n  geom_histogram(bins=30, color=\"black\", fill=\"grey\") +\n  labs(\n    title = \"Number of parks established in Nashville per year\",\n    subtitle = \"\",\n    x= \"Year Established\",\n    y= \"Frequency\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12)\n\n\n\n\nThe rate of new park development looks to be increasing over time. An ECDF plot supports this observation.\n\nCodedf %>%\n  ggplot()+\n  aes(year_established) +\n  stat_ecdf() +\n  labs(\n    title = \"Cumulative distribution of Nashville parks over time\",\n    subtitle = \"\",\n    x= \"Year Established\",\n    y= \"ECDF\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12)"
  },
  {
    "objectID": "posts/gganimate-map/index.html#creating-the-animation",
    "href": "posts/gganimate-map/index.html#creating-the-animation",
    "title": "Using gganimate to create cartographic animations",
    "section": "Creating the Animation",
    "text": "Creating the Animation\nNow that we’ve cleaned and explored the data, let’s create an animation with gganimate and ggmap to visualize the development of Nashville’s parks by year.\nFirst, let’s try to just plot the locations of all the parks. Luckily, the (latitude, longitude) coordinates are provided.\nWe start with the qmplot() function from ggmap, which is a shortcut for plotting on maps, just like qplot() in ggplot2. We pass in the latitude and longitude coordinates and the data frame. The argument maptype = \"toner-lite\" indicates the type of basemap to use as the background. We also specify alpha=0.5 so we can see when the points overlap. I would like larger parks to be represented by larger circles, so we can map size to acreage by aes(size=acres). Then we add the extra theming using the cowplot package.\n\nCodeqmplot(long, lat, data = df, maptype=\"toner-lite\", alpha=0.5) + \n  aes(size=acres) +\n  labs(\n    title = \"Nashville Parks\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\nLooks pretty good already. Now let’s make this into an animation!\nWe will add the transition_states() function from gganimate and specify that each state of the animation is determined by year_established. We also set subtitle = \"Year: {closest_state}\" to display the year of the current frame.\n\nCodeqmplot(x=long, y=lat, data = df) + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established)\n\n\n\n\nIt’s animated now, but there are two problems.\nFirst, the points are disappearing after each year. We can add shadow_mark(color=\"black\") to have the points stay on the plot. We specify that the old points are colored black so that we can color the current points red, to highlight which points were just displayed.\nSecond, the passage of time is not constant. We want to have each frame change in increments of one year. In our current animation, the years are skipping between the years present in the data. To fix this, we convert year_established to a factor, and fill in the missing years.\n\nCodedf$year_established <-\n  df$year_established %>%\n  # convert to factor\n  as.factor() %>%\n  # add extra years\n  fct_expand(1900:2019 %>% as.character) %>%\n  # sort years\n  fct_relevel(1900:2019 %>% as.character)\n\n\nNow that we’ve made those changes, let’s try again.\n\nCodeqmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\n\n\n\nLooks good! But wait a second… the animation only goes to 1950. Wasn’t it supposed to go to 2015? This is a little quirk of gganimate. By default, the animation is capped at 100 frames. For the transition_states() animation, by default a single frame is allocated for each state, and another frame is allocated for transitions between states. So 100 frames can represent 50 years of data. The animation is cut short because we have more than 50 years of data.\nLet’s fix this by saving the animation to a variable, and then using the animate() function to increase the number of frames.\n\nCodeparks_anim <- \n  qmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 12) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\nanimate(\n  parks_anim,\n  nframes=300, # number of frames to compute\n  )\n\n\n\n\nMy preferred method of rendering the animation is to use ffmpeg, instead of the default GIF renderer, because it creates videos (.mp4) rather than GIFs. You will need to install ffpmeg on your computer separately. Using ffmpeg also allows for finer control over the frame rate of the animation and creates smaller files. I’ll show how to use it below.\nThe animate() function has parameters for duration (total duration in seconds), fps (frames per second), and nframes (total number of frames). You can specify any two. For our case, we give the duration and number of frames, and gganimate figures out the proper frame rate to fit the specified number of frames into the specified number of seconds.\nWe also set res=300 to increase the resolution. This has the side effect of making the font appear larger, so we decrease the font size in the call to theme_cowplot().\nBe warned that this may take a bit of time to animate. Here’s the final result!\n\nCodeparks_anim <- \n  qmplot(long, lat, data = df,\n       maptype = \"toner-lite\", alpha=0.5, color=\"red\") + \n  aes(size=acres, group=year_established) +\n  labs(\n    title = \"Nashville Parks\",\n    subtitle = \"Year: {closest_state}\",\n    x= \"Longitude\",\n    y= \"Latitude\",\n    caption = \"Area corresponds to acreage \\n Data available from Nashville Open Data\") +\n  cowplot::theme_cowplot(font_family = \"Source Sans Pro\",\n                         font_size = 10) +\n  theme(legend.position = \"none\") +\n  transition_states(year_established) +\n  shadow_mark(color=\"black\")\n\nanimate(\n  parks_anim,\n  duration=15, # duration of the animation in seconds\n  nframes=768, # number of frames to compute\n  height = 6,\n  width = 6,\n  units = \"in\",\n  res = 300, # resolution of the output\n  renderer = ffmpeg_renderer() # render to video with ffmpeg\n  )"
  },
  {
    "objectID": "posts/gradient-descent-pt1/index.html",
    "href": "posts/gradient-descent-pt1/index.html",
    "title": "What is Gradient Descent? (Part I)",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(glue)\nlibrary(gganimate)"
  },
  {
    "objectID": "posts/gradient-descent-pt1/index.html#introduction",
    "href": "posts/gradient-descent-pt1/index.html#introduction",
    "title": "What is Gradient Descent? (Part I)",
    "section": "Introduction",
    "text": "Introduction\nGradient descent is an optimization algorithm that finds the minimum of a function. Commonly, the function to be minimized is a loss function: a function that quantifies the “badness” associated with the given inputs, which you would naturally want to minimize. A common loss function is the mean-squared error. For example, using mean-squared error, the loss incurred by an inaccurate prediction is the squared distance from the prediction to the true value. Neural networks are commonly optimized using some form of gradient descent.\nLet’s start with a simple example, where we already know the answer. We wish to minimize the quadratic function given by\n\\[\nf(x) = (x + 2)^2 + 3\n\\]\nThe shape of the function, a parabola, is shown in the plot below. Most applications of gradient descent occur in dimensions much higher than 2D, where we cannot so easily visualize the function we are trying to minimize.\n\nCodex <- seq(-10,5,length.out=1e4)\ny <- (x + 2)^2 + 3\n\nplot_data <- tibble(x, y)\n\nplot_data %>%\n  ggplot() +\n  aes(x=x, y=y) +\n  geom_line() +\n  geom_hline(yintercept = 3, linetype=2) +\n  geom_vline(xintercept = -2, linetype=2) +\n  scale_x_continuous(breaks=c(-2)) +\n  scale_y_continuous(breaks=c(3)) +\n  cowplot::theme_cowplot(font_family = \"Lato\")\n\n\n\n\nFinding the minimum is a solved problem using calculus. We can take the first-derivative, set it equal to zero, and solve, to obtain a minimum of \\(y=3\\), which occurs at \\(x=-2\\). Then an application of the second-derivative test confirms that it is minimum, rather than a maximum. Our goal is to reproduce this result using gradient descent."
  },
  {
    "objectID": "posts/gradient-descent-pt1/index.html#an-analogy-for-gradient-descent",
    "href": "posts/gradient-descent-pt1/index.html#an-analogy-for-gradient-descent",
    "title": "What is Gradient Descent? (Part I)",
    "section": "An analogy for gradient descent",
    "text": "An analogy for gradient descent\nGradient descent works by starting at a location in the space. Then, each iteration of the algorithm it moves downhill with respect to the function, which is by definition opposite the gradient. The algorithm proceeds downhill until it reaches a minimum where the gradient is zero within some tolerance (success) or the maximum number of iterations is reached (failure). Apart from the tuning parameters of the algorithm, which we will discuss later, the only information gradient descent needs to work is the function to be minimized and its first derivative.\nHere’s an analogy. Think of a ball moving under the influence of gravity in a landscape of hills and valleys. If you let the ball move freely, it will roll to a point of minimum height in the landscape. Does the ball know the whole landscape and decide to move to the minimum point? No. The only information it uses to find the minimum is the slope at the point it is currently at. The local information is enough. Gravity is constantly moving the ball downhill, based on the slope of the landscape at the current location."
  },
  {
    "objectID": "posts/gradient-descent-pt1/index.html#the-gradient-descent-algorithm",
    "href": "posts/gradient-descent-pt1/index.html#the-gradient-descent-algorithm",
    "title": "What is Gradient Descent? (Part I)",
    "section": "The gradient descent algorithm",
    "text": "The gradient descent algorithm\nThe general algorithm for gradient descent is as follows:\n\nPick a starting point and a learning rate\nUsing the derivative of the function, compute the gradient (i.e., slope) at the current point.\nCompute the step size: \\(\\text{delta} = - \\text{gradient} * \\text{learning\\_rate}\\)\n\nSet \\(x \\rightarrow x + \\text{delta}\\)\n\nRepeat from step 2 until either delta is below a certain threshold or a maximum number of iterations is reached\n\nNow, let’s go through this step-by-step for our quadratic function example.\n\nPick an arbitrarily chosen starting point of \\(x=5\\). Thus \\(f(x) = (5 + 2)^2 + 3 = 52\\). We also pick a commonly used learning rate of 0.1.\nThe derivative of \\(f(x)\\) is \\(\\frac{df}{dx} = 2x+4\\). So the gradient is \\(2(5) + 4 = \\boxed{14}\\)\n\nSet the step size: \\(\\text{delta} = - \\text{gradient} * \\text{learning\\_rate} = - 14* 0.01 = \\boxed{-0.14}\\)\n\nSet the current value of \\(x\\) to \\(x + delta = 5 - 0.14 = \\boxed{4.86}\\)\n\nAssume we set the step size (delta) threshold to 0.001 and the maximum number of iteration to 5000. Since neither of these criteria are currently met, we go back to step 2, but now with \\(x=4.86\\), and repeat until we meet one of the exit conditions."
  },
  {
    "objectID": "posts/gradient-descent-pt1/index.html#implementation-in-r",
    "href": "posts/gradient-descent-pt1/index.html#implementation-in-r",
    "title": "What is Gradient Descent? (Part I)",
    "section": "Implementation in R",
    "text": "Implementation in R\nNow that we understand the gradient descent algorithm in theory, let’s translate this into R code.\n\nCode# Define f(x) and df/dx\nf <- function(x){(x+2)^2 + 3}\ndf_dx <- function(x){2*x+4}\n\n# Set learning rate\nlearning_rate <- 0.1\n\n# Set starting point\nx <- 5\n\n# Create a counter to track the iteration\niter <- 1\n\nwhile(TRUE){ # Loop until we reach exit conditions\n  \n  # Compute the gradient at the current x value\n  current_grad <- df_dx(x)\n  \n  # Compute delta at the current x value\n  delta <- -current_grad*learning_rate\n  \n  # Compute the updated x value, given delta\n  x <- x + delta\n  \n  # Print the current state of the algorithm\n  # the glue package is used for printing variables easily\n  print(glue(\"Iteration: {iter}\"))\n  print(glue(\"x: {x}\"))\n  print(glue(\"y: {f(x)}\"))\n  print(glue(\"delta: {delta}\"))\n  \n  # Increment the iteration counter\n  iter <- iter + 1\n  \n  # Exit if delta is below the threshold or max iterations have been reached\n  if (abs(delta)<0.001 | iter>5000) {\n    break\n  }\n}\n\n\nHere’s the output from the beginning and end of the algorithm.\nIteration: 1\nx: 3.6\ny: 34.36\ndelta: -1.4\nIteration: 2\nx: 2.48\ny: 23.0704\ndelta: -1.12\nIteration: 3\nx: 1.584\ny: 15.845056\ndelta: -0.896\n\n...\n\nIteration: 32\nx: -1.994454028624\ny: 3.0000307577985\ndelta: -0.00138649284399963\nIteration: 33\nx: -1.9955632228992\ny: 3.00001968499104\ndelta: -0.00110919427519969\nIteration: 34\nx: -1.99645057831936\ny: 3.00001259839427\ndelta: -0.000887355420159741\n\nWe see that our gradient descent algorithm converged to the minimum at \\((3,-2)\\), with some error that could be reduced if we lowered the step size threshold."
  },
  {
    "objectID": "posts/gradient-descent-pt1/index.html#animations",
    "href": "posts/gradient-descent-pt1/index.html#animations",
    "title": "What is Gradient Descent? (Part I)",
    "section": "Animations",
    "text": "Animations\nAnimations are a good way to get intuition on how optimization algorithms like gradient descent work. The animation below shows our algorithm using three different learning rates. The code to create the animations can be found here.\n\nVideo\n\nUsing a learning rate of 0.01 takes much longer to converge, but with more complicated functions it is less likely to overshoot and miss the minimum. Using a learning rate of 0.95, the algorithm constantly overshoots the minimum and oscillates on either side of it until it finally settles down. A learning rate of 0.1 seems like the best compromise between accuracy and speed, since we know the true minimum.\n\n\nVideo\nSpecial case is when the learning rate is exactly 1: the algorithm will move between two points on opposite sides on the parabola and remain stuck there\nIn this specific example, a learning rate higher than 1 will constantly overshoot the minimum and will never converge. A special case is when the learning rate is exactly 1: the algorithm will move between two points on opposite sides on the parabola and remain stuck there. See the animation below.\nIn real applications, where the true minimum is unknown, trial and error is necessary to find a good learning rate. There are more complex algorithms that build on gradient descent to automatically tune the learning rate as the algorithm progresses."
  },
  {
    "objectID": "posts/gradient-descent-pt1/index.html#next-steps",
    "href": "posts/gradient-descent-pt1/index.html#next-steps",
    "title": "What is Gradient Descent? (Part I)",
    "section": "Next steps",
    "text": "Next steps\nIn the next post in this series, we will extend our gradient descent algorithm to optimize over more complex functions: fitting a least-squares regression line, and a logistic regression curve."
  },
  {
    "objectID": "posts/linear-algebra/index.html",
    "href": "posts/linear-algebra/index.html",
    "title": "Linear Algebra / Matrix Operations in R",
    "section": "",
    "text": "The following material should be understandable to anyone with a basic knowledge of R and linear algebra. I plan to update this page with more material in the future.\nCreating vectors\nVectors can be created with the c() function.\n\nCodec(1,3,7)\n\n[1] 1 3 7\n\n\nFor vectors that are a continuous sequence of numbers, you can use the : operator.\n\nCode1:8\n\n[1] 1 2 3 4 5 6 7 8\n\n\nFor vectors of repeated numbers, such as a zero vector, use the rep() function.\n\nCode# Create a zero vector of length 10\nrep(0, times=10)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\nCode# Or to be more concise\n# rep(0, 10)\n\n\nMore complicated sequences can be created with the seq() function.\n\nCode# Create a sequence from 1 to 3 with a step size of 0.5\nseq(1,3, by = 0.5)\n\n[1] 1.0 1.5 2.0 2.5 3.0\n\n\n\nCode# Create 10 equal spaced numbers between 1 and 3\nseq(1,3, length.out = 10)\n\n [1] 1.000000 1.222222 1.444444 1.666667 1.888889 2.111111 2.333333 2.555556\n [9] 2.777778 3.000000\n\n\nCreating matrices\nThere are two common ways to create matrices in R.\nThe first method is using the matrix() function. You pass the elements of the matrix into matrix() and specify the number of rows and columns. Note that R fills in the numbers going down each column. This can be unintuitive.\n\nCode# Put the numbers 1 to 12 in 3 rows and 4 columns\nmatrix(1:12, nrow=3, ncol=4)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\n\nCode# Put the numbers 1 to 12 in 2 rows and 6 columns\nmatrix(1:12, nrow=2, ncol=6)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    3    5    7    9   11\n[2,]    2    4    6    8   10   12\n\n\nThe second method is using rbind() and cbind(). Use rbind() to combine vectors into a matrix by row, and cbind() to combine vectors into a matrix by column.\n\nCodex1 <- c(1,2,3,4)\nx2 <- c(5,6,7,8)\nx3 <- c(9,10,11,12) \n\n\n\nCoderbind(x1, x2, x3)\n\n   [,1] [,2] [,3] [,4]\nx1    1    2    3    4\nx2    5    6    7    8\nx3    9   10   11   12\n\n\n\nCodecbind(x1, x2, x3)\n\n     x1 x2 x3\n[1,]  1  5  9\n[2,]  2  6 10\n[3,]  3  7 11\n[4,]  4  8 12\n\n\nTo create a \\(n \\times n\\) identity matrix, use the diag(n) function.\n\nCodediag(5)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    0    0    0    0\n[2,]    0    1    0    0    0\n[3,]    0    0    1    0    0\n[4,]    0    0    0    1    0\n[5,]    0    0    0    0    1\n\n\nOnce a matrix is created, the dim() can be used to obtain the dimensions.\n\nCodem <- cbind(x1, x2, x3)\n\n# See that m has 4 rows and 3 columns\ndim(m)\n\n[1] 4 3\n\n\nIf you are only interested in the number of rows and columns, use nrow() and ncol() respectively.\n\nCodenrow(m)\n\n[1] 4\n\nCodencol(m)\n\n[1] 3\n\n\nMatrix indexing\nOnce you have a matrix in R, how do you subset parts of the matrix? Let’s use this matrix, m as an example.\n\nCodem <- matrix(1:12, nrow=3, ncol=4)\n\nprint(m)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe syntax m[i] selects the ith element. Recall that R counts going down columns.\n\nCodem[2]\n\n[1] 2\n\n\nThe syntax m[i, ] selects the ith row\n\nCodem[2,]\n\n[1]  2  5  8 11\n\n\nThe syntax m[,i] selects the ith column.\n\nCodem[,2]\n\n[1] 4 5 6\n\n\nThe syntax m[i,j] selects the element in the ith row and jth column.\n\nCodem[2,2]\n\n[1] 5\n\n\nMatrix operations\nMatrix multiplication uses the %*% operator.\n\nCode# Define matrices\nA <- matrix(1:9, nrow=3, ncol=3)\nB <- matrix(1:6, nrow=3, ncol=2)\n\n# Matrix multiplication\nA %*% B\n\n     [,1] [,2]\n[1,]   30   66\n[2,]   36   81\n[3,]   42   96\n\n\nRemember that the order of matrix multiplication is important!\n\nCodeB %*% A\n\n\nError in B %*% A : non-conformable arguments"
  },
  {
    "objectID": "posts/logistic-regresstion-by-hand/index.html",
    "href": "posts/logistic-regresstion-by-hand/index.html",
    "title": "Logistic regression (by hand)",
    "section": "",
    "text": "Code# Load packages\nlibrary(tidyverse)\nlibrary(gganimate)\n\nlibrary(Hmisc)\n\nlibrary(palmerpenguins)\n\nlibrary(patchwork)\nlibrary(kableExtra)\nlibrary(glue)\n\n# Set global ggplot theme\ntheme_set(cowplot::theme_cowplot(font_size=14,\n                                 font_family = \"Source Sans Pro\"))"
  },
  {
    "objectID": "posts/logistic-regresstion-by-hand/index.html#exploratory-data-analysis",
    "href": "posts/logistic-regresstion-by-hand/index.html#exploratory-data-analysis",
    "title": "Logistic regression (by hand)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nYou can explore the raw data below.\n\n\n\n\n\n species \n    adelie \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n  \n\n\n Adelie \n    1 \n    39.1 \n    18.7 \n    181 \n    3750 \n  \n\n Adelie \n    1 \n    39.5 \n    17.4 \n    186 \n    3800 \n  \n\n Adelie \n    1 \n    40.3 \n    18.0 \n    195 \n    3250 \n  \n\n Adelie \n    1 \n    36.7 \n    19.3 \n    193 \n    3450 \n  \n\n Adelie \n    1 \n    39.3 \n    20.6 \n    190 \n    3650 \n  \n\n Adelie \n    1 \n    38.9 \n    17.8 \n    181 \n    3625 \n  \n\n Adelie \n    1 \n    39.2 \n    19.6 \n    195 \n    4675 \n  \n\n Adelie \n    1 \n    34.1 \n    18.1 \n    193 \n    3475 \n  \n\n Adelie \n    1 \n    42.0 \n    20.2 \n    190 \n    4250 \n  \n\n Adelie \n    1 \n    37.8 \n    17.1 \n    186 \n    3300 \n  \n\n Adelie \n    1 \n    37.8 \n    17.3 \n    180 \n    3700 \n  \n\n Adelie \n    1 \n    41.1 \n    17.6 \n    182 \n    3200 \n  \n\n Adelie \n    1 \n    38.6 \n    21.2 \n    191 \n    3800 \n  \n\n Adelie \n    1 \n    34.6 \n    21.1 \n    198 \n    4400 \n  \n\n Adelie \n    1 \n    36.6 \n    17.8 \n    185 \n    3700 \n  \n\n Adelie \n    1 \n    38.7 \n    19.0 \n    195 \n    3450 \n  \n\n Adelie \n    1 \n    42.5 \n    20.7 \n    197 \n    4500 \n  \n\n Adelie \n    1 \n    34.4 \n    18.4 \n    184 \n    3325 \n  \n\n Adelie \n    1 \n    46.0 \n    21.5 \n    194 \n    4200 \n  \n\n Adelie \n    1 \n    37.8 \n    18.3 \n    174 \n    3400 \n  \n\n Adelie \n    1 \n    37.7 \n    18.7 \n    180 \n    3600 \n  \n\n Adelie \n    1 \n    35.9 \n    19.2 \n    189 \n    3800 \n  \n\n Adelie \n    1 \n    38.2 \n    18.1 \n    185 \n    3950 \n  \n\n Adelie \n    1 \n    38.8 \n    17.2 \n    180 \n    3800 \n  \n\n Adelie \n    1 \n    35.3 \n    18.9 \n    187 \n    3800 \n  \n\n Adelie \n    1 \n    40.6 \n    18.6 \n    183 \n    3550 \n  \n\n Adelie \n    1 \n    40.5 \n    17.9 \n    187 \n    3200 \n  \n\n Adelie \n    1 \n    37.9 \n    18.6 \n    172 \n    3150 \n  \n\n Adelie \n    1 \n    40.5 \n    18.9 \n    180 \n    3950 \n  \n\n Adelie \n    1 \n    39.5 \n    16.7 \n    178 \n    3250 \n  \n\n Adelie \n    1 \n    37.2 \n    18.1 \n    178 \n    3900 \n  \n\n Adelie \n    1 \n    39.5 \n    17.8 \n    188 \n    3300 \n  \n\n Adelie \n    1 \n    40.9 \n    18.9 \n    184 \n    3900 \n  \n\n Adelie \n    1 \n    36.4 \n    17.0 \n    195 \n    3325 \n  \n\n Adelie \n    1 \n    39.2 \n    21.1 \n    196 \n    4150 \n  \n\n Adelie \n    1 \n    38.8 \n    20.0 \n    190 \n    3950 \n  \n\n Adelie \n    1 \n    42.2 \n    18.5 \n    180 \n    3550 \n  \n\n Adelie \n    1 \n    37.6 \n    19.3 \n    181 \n    3300 \n  \n\n Adelie \n    1 \n    39.8 \n    19.1 \n    184 \n    4650 \n  \n\n Adelie \n    1 \n    36.5 \n    18.0 \n    182 \n    3150 \n  \n\n Adelie \n    1 \n    40.8 \n    18.4 \n    195 \n    3900 \n  \n\n Adelie \n    1 \n    36.0 \n    18.5 \n    186 \n    3100 \n  \n\n Adelie \n    1 \n    44.1 \n    19.7 \n    196 \n    4400 \n  \n\n Adelie \n    1 \n    37.0 \n    16.9 \n    185 \n    3000 \n  \n\n Adelie \n    1 \n    39.6 \n    18.8 \n    190 \n    4600 \n  \n\n Adelie \n    1 \n    41.1 \n    19.0 \n    182 \n    3425 \n  \n\n Adelie \n    1 \n    37.5 \n    18.9 \n    179 \n    2975 \n  \n\n Adelie \n    1 \n    36.0 \n    17.9 \n    190 \n    3450 \n  \n\n Adelie \n    1 \n    42.3 \n    21.2 \n    191 \n    4150 \n  \n\n Adelie \n    1 \n    39.6 \n    17.7 \n    186 \n    3500 \n  \n\n Adelie \n    1 \n    40.1 \n    18.9 \n    188 \n    4300 \n  \n\n Adelie \n    1 \n    35.0 \n    17.9 \n    190 \n    3450 \n  \n\n Adelie \n    1 \n    42.0 \n    19.5 \n    200 \n    4050 \n  \n\n Adelie \n    1 \n    34.5 \n    18.1 \n    187 \n    2900 \n  \n\n Adelie \n    1 \n    41.4 \n    18.6 \n    191 \n    3700 \n  \n\n Adelie \n    1 \n    39.0 \n    17.5 \n    186 \n    3550 \n  \n\n Adelie \n    1 \n    40.6 \n    18.8 \n    193 \n    3800 \n  \n\n Adelie \n    1 \n    36.5 \n    16.6 \n    181 \n    2850 \n  \n\n Adelie \n    1 \n    37.6 \n    19.1 \n    194 \n    3750 \n  \n\n Adelie \n    1 \n    35.7 \n    16.9 \n    185 \n    3150 \n  \n\n Adelie \n    1 \n    41.3 \n    21.1 \n    195 \n    4400 \n  \n\n Adelie \n    1 \n    37.6 \n    17.0 \n    185 \n    3600 \n  \n\n Adelie \n    1 \n    41.1 \n    18.2 \n    192 \n    4050 \n  \n\n Adelie \n    1 \n    36.4 \n    17.1 \n    184 \n    2850 \n  \n\n Adelie \n    1 \n    41.6 \n    18.0 \n    192 \n    3950 \n  \n\n Adelie \n    1 \n    35.5 \n    16.2 \n    195 \n    3350 \n  \n\n Adelie \n    1 \n    41.1 \n    19.1 \n    188 \n    4100 \n  \n\n Adelie \n    1 \n    35.9 \n    16.6 \n    190 \n    3050 \n  \n\n Adelie \n    1 \n    41.8 \n    19.4 \n    198 \n    4450 \n  \n\n Adelie \n    1 \n    33.5 \n    19.0 \n    190 \n    3600 \n  \n\n Adelie \n    1 \n    39.7 \n    18.4 \n    190 \n    3900 \n  \n\n Adelie \n    1 \n    39.6 \n    17.2 \n    196 \n    3550 \n  \n\n Adelie \n    1 \n    45.8 \n    18.9 \n    197 \n    4150 \n  \n\n Adelie \n    1 \n    35.5 \n    17.5 \n    190 \n    3700 \n  \n\n Adelie \n    1 \n    42.8 \n    18.5 \n    195 \n    4250 \n  \n\n Adelie \n    1 \n    40.9 \n    16.8 \n    191 \n    3700 \n  \n\n Adelie \n    1 \n    37.2 \n    19.4 \n    184 \n    3900 \n  \n\n Adelie \n    1 \n    36.2 \n    16.1 \n    187 \n    3550 \n  \n\n Adelie \n    1 \n    42.1 \n    19.1 \n    195 \n    4000 \n  \n\n Adelie \n    1 \n    34.6 \n    17.2 \n    189 \n    3200 \n  \n\n Adelie \n    1 \n    42.9 \n    17.6 \n    196 \n    4700 \n  \n\n Adelie \n    1 \n    36.7 \n    18.8 \n    187 \n    3800 \n  \n\n Adelie \n    1 \n    35.1 \n    19.4 \n    193 \n    4200 \n  \n\n Adelie \n    1 \n    37.3 \n    17.8 \n    191 \n    3350 \n  \n\n Adelie \n    1 \n    41.3 \n    20.3 \n    194 \n    3550 \n  \n\n Adelie \n    1 \n    36.3 \n    19.5 \n    190 \n    3800 \n  \n\n Adelie \n    1 \n    36.9 \n    18.6 \n    189 \n    3500 \n  \n\n Adelie \n    1 \n    38.3 \n    19.2 \n    189 \n    3950 \n  \n\n Adelie \n    1 \n    38.9 \n    18.8 \n    190 \n    3600 \n  \n\n Adelie \n    1 \n    35.7 \n    18.0 \n    202 \n    3550 \n  \n\n Adelie \n    1 \n    41.1 \n    18.1 \n    205 \n    4300 \n  \n\n Adelie \n    1 \n    34.0 \n    17.1 \n    185 \n    3400 \n  \n\n Adelie \n    1 \n    39.6 \n    18.1 \n    186 \n    4450 \n  \n\n Adelie \n    1 \n    36.2 \n    17.3 \n    187 \n    3300 \n  \n\n Adelie \n    1 \n    40.8 \n    18.9 \n    208 \n    4300 \n  \n\n Adelie \n    1 \n    38.1 \n    18.6 \n    190 \n    3700 \n  \n\n Adelie \n    1 \n    40.3 \n    18.5 \n    196 \n    4350 \n  \n\n Adelie \n    1 \n    33.1 \n    16.1 \n    178 \n    2900 \n  \n\n Adelie \n    1 \n    43.2 \n    18.5 \n    192 \n    4100 \n  \n\n Adelie \n    1 \n    35.0 \n    17.9 \n    192 \n    3725 \n  \n\n Adelie \n    1 \n    41.0 \n    20.0 \n    203 \n    4725 \n  \n\n Adelie \n    1 \n    37.7 \n    16.0 \n    183 \n    3075 \n  \n\n Adelie \n    1 \n    37.8 \n    20.0 \n    190 \n    4250 \n  \n\n Adelie \n    1 \n    37.9 \n    18.6 \n    193 \n    2925 \n  \n\n Adelie \n    1 \n    39.7 \n    18.9 \n    184 \n    3550 \n  \n\n Adelie \n    1 \n    38.6 \n    17.2 \n    199 \n    3750 \n  \n\n Adelie \n    1 \n    38.2 \n    20.0 \n    190 \n    3900 \n  \n\n Adelie \n    1 \n    38.1 \n    17.0 \n    181 \n    3175 \n  \n\n Adelie \n    1 \n    43.2 \n    19.0 \n    197 \n    4775 \n  \n\n Adelie \n    1 \n    38.1 \n    16.5 \n    198 \n    3825 \n  \n\n Adelie \n    1 \n    45.6 \n    20.3 \n    191 \n    4600 \n  \n\n Adelie \n    1 \n    39.7 \n    17.7 \n    193 \n    3200 \n  \n\n Adelie \n    1 \n    42.2 \n    19.5 \n    197 \n    4275 \n  \n\n Adelie \n    1 \n    39.6 \n    20.7 \n    191 \n    3900 \n  \n\n Adelie \n    1 \n    42.7 \n    18.3 \n    196 \n    4075 \n  \n\n Adelie \n    1 \n    38.6 \n    17.0 \n    188 \n    2900 \n  \n\n Adelie \n    1 \n    37.3 \n    20.5 \n    199 \n    3775 \n  \n\n Adelie \n    1 \n    35.7 \n    17.0 \n    189 \n    3350 \n  \n\n Adelie \n    1 \n    41.1 \n    18.6 \n    189 \n    3325 \n  \n\n Adelie \n    1 \n    36.2 \n    17.2 \n    187 \n    3150 \n  \n\n Adelie \n    1 \n    37.7 \n    19.8 \n    198 \n    3500 \n  \n\n Adelie \n    1 \n    40.2 \n    17.0 \n    176 \n    3450 \n  \n\n Adelie \n    1 \n    41.4 \n    18.5 \n    202 \n    3875 \n  \n\n Adelie \n    1 \n    35.2 \n    15.9 \n    186 \n    3050 \n  \n\n Adelie \n    1 \n    40.6 \n    19.0 \n    199 \n    4000 \n  \n\n Adelie \n    1 \n    38.8 \n    17.6 \n    191 \n    3275 \n  \n\n Adelie \n    1 \n    41.5 \n    18.3 \n    195 \n    4300 \n  \n\n Adelie \n    1 \n    39.0 \n    17.1 \n    191 \n    3050 \n  \n\n Adelie \n    1 \n    44.1 \n    18.0 \n    210 \n    4000 \n  \n\n Adelie \n    1 \n    38.5 \n    17.9 \n    190 \n    3325 \n  \n\n Adelie \n    1 \n    43.1 \n    19.2 \n    197 \n    3500 \n  \n\n Adelie \n    1 \n    36.8 \n    18.5 \n    193 \n    3500 \n  \n\n Adelie \n    1 \n    37.5 \n    18.5 \n    199 \n    4475 \n  \n\n Adelie \n    1 \n    38.1 \n    17.6 \n    187 \n    3425 \n  \n\n Adelie \n    1 \n    41.1 \n    17.5 \n    190 \n    3900 \n  \n\n Adelie \n    1 \n    35.6 \n    17.5 \n    191 \n    3175 \n  \n\n Adelie \n    1 \n    40.2 \n    20.1 \n    200 \n    3975 \n  \n\n Adelie \n    1 \n    37.0 \n    16.5 \n    185 \n    3400 \n  \n\n Adelie \n    1 \n    39.7 \n    17.9 \n    193 \n    4250 \n  \n\n Adelie \n    1 \n    40.2 \n    17.1 \n    193 \n    3400 \n  \n\n Adelie \n    1 \n    40.6 \n    17.2 \n    187 \n    3475 \n  \n\n Adelie \n    1 \n    32.1 \n    15.5 \n    188 \n    3050 \n  \n\n Adelie \n    1 \n    40.7 \n    17.0 \n    190 \n    3725 \n  \n\n Adelie \n    1 \n    37.3 \n    16.8 \n    192 \n    3000 \n  \n\n Adelie \n    1 \n    39.0 \n    18.7 \n    185 \n    3650 \n  \n\n Adelie \n    1 \n    39.2 \n    18.6 \n    190 \n    4250 \n  \n\n Adelie \n    1 \n    36.6 \n    18.4 \n    184 \n    3475 \n  \n\n Adelie \n    1 \n    36.0 \n    17.8 \n    195 \n    3450 \n  \n\n Adelie \n    1 \n    37.8 \n    18.1 \n    193 \n    3750 \n  \n\n Adelie \n    1 \n    36.0 \n    17.1 \n    187 \n    3700 \n  \n\n Adelie \n    1 \n    41.5 \n    18.5 \n    201 \n    4000 \n  \n\n Gentoo \n    0 \n    46.1 \n    13.2 \n    211 \n    4500 \n  \n\n Gentoo \n    0 \n    50.0 \n    16.3 \n    230 \n    5700 \n  \n\n Gentoo \n    0 \n    48.7 \n    14.1 \n    210 \n    4450 \n  \n\n Gentoo \n    0 \n    50.0 \n    15.2 \n    218 \n    5700 \n  \n\n Gentoo \n    0 \n    47.6 \n    14.5 \n    215 \n    5400 \n  \n\n Gentoo \n    0 \n    46.5 \n    13.5 \n    210 \n    4550 \n  \n\n Gentoo \n    0 \n    45.4 \n    14.6 \n    211 \n    4800 \n  \n\n Gentoo \n    0 \n    46.7 \n    15.3 \n    219 \n    5200 \n  \n\n Gentoo \n    0 \n    43.3 \n    13.4 \n    209 \n    4400 \n  \n\n Gentoo \n    0 \n    46.8 \n    15.4 \n    215 \n    5150 \n  \n\n Gentoo \n    0 \n    40.9 \n    13.7 \n    214 \n    4650 \n  \n\n Gentoo \n    0 \n    49.0 \n    16.1 \n    216 \n    5550 \n  \n\n Gentoo \n    0 \n    45.5 \n    13.7 \n    214 \n    4650 \n  \n\n Gentoo \n    0 \n    48.4 \n    14.6 \n    213 \n    5850 \n  \n\n Gentoo \n    0 \n    45.8 \n    14.6 \n    210 \n    4200 \n  \n\n Gentoo \n    0 \n    49.3 \n    15.7 \n    217 \n    5850 \n  \n\n Gentoo \n    0 \n    42.0 \n    13.5 \n    210 \n    4150 \n  \n\n Gentoo \n    0 \n    49.2 \n    15.2 \n    221 \n    6300 \n  \n\n Gentoo \n    0 \n    46.2 \n    14.5 \n    209 \n    4800 \n  \n\n Gentoo \n    0 \n    48.7 \n    15.1 \n    222 \n    5350 \n  \n\n Gentoo \n    0 \n    50.2 \n    14.3 \n    218 \n    5700 \n  \n\n Gentoo \n    0 \n    45.1 \n    14.5 \n    215 \n    5000 \n  \n\n Gentoo \n    0 \n    46.5 \n    14.5 \n    213 \n    4400 \n  \n\n Gentoo \n    0 \n    46.3 \n    15.8 \n    215 \n    5050 \n  \n\n Gentoo \n    0 \n    42.9 \n    13.1 \n    215 \n    5000 \n  \n\n Gentoo \n    0 \n    46.1 \n    15.1 \n    215 \n    5100 \n  \n\n Gentoo \n    0 \n    44.5 \n    14.3 \n    216 \n    4100 \n  \n\n Gentoo \n    0 \n    47.8 \n    15.0 \n    215 \n    5650 \n  \n\n Gentoo \n    0 \n    48.2 \n    14.3 \n    210 \n    4600 \n  \n\n Gentoo \n    0 \n    50.0 \n    15.3 \n    220 \n    5550 \n  \n\n Gentoo \n    0 \n    47.3 \n    15.3 \n    222 \n    5250 \n  \n\n Gentoo \n    0 \n    42.8 \n    14.2 \n    209 \n    4700 \n  \n\n Gentoo \n    0 \n    45.1 \n    14.5 \n    207 \n    5050 \n  \n\n Gentoo \n    0 \n    59.6 \n    17.0 \n    230 \n    6050 \n  \n\n Gentoo \n    0 \n    49.1 \n    14.8 \n    220 \n    5150 \n  \n\n Gentoo \n    0 \n    48.4 \n    16.3 \n    220 \n    5400 \n  \n\n Gentoo \n    0 \n    42.6 \n    13.7 \n    213 \n    4950 \n  \n\n Gentoo \n    0 \n    44.4 \n    17.3 \n    219 \n    5250 \n  \n\n Gentoo \n    0 \n    44.0 \n    13.6 \n    208 \n    4350 \n  \n\n Gentoo \n    0 \n    48.7 \n    15.7 \n    208 \n    5350 \n  \n\n Gentoo \n    0 \n    42.7 \n    13.7 \n    208 \n    3950 \n  \n\n Gentoo \n    0 \n    49.6 \n    16.0 \n    225 \n    5700 \n  \n\n Gentoo \n    0 \n    45.3 \n    13.7 \n    210 \n    4300 \n  \n\n Gentoo \n    0 \n    49.6 \n    15.0 \n    216 \n    4750 \n  \n\n Gentoo \n    0 \n    50.5 \n    15.9 \n    222 \n    5550 \n  \n\n Gentoo \n    0 \n    43.6 \n    13.9 \n    217 \n    4900 \n  \n\n Gentoo \n    0 \n    45.5 \n    13.9 \n    210 \n    4200 \n  \n\n Gentoo \n    0 \n    50.5 \n    15.9 \n    225 \n    5400 \n  \n\n Gentoo \n    0 \n    44.9 \n    13.3 \n    213 \n    5100 \n  \n\n Gentoo \n    0 \n    45.2 \n    15.8 \n    215 \n    5300 \n  \n\n Gentoo \n    0 \n    46.6 \n    14.2 \n    210 \n    4850 \n  \n\n Gentoo \n    0 \n    48.5 \n    14.1 \n    220 \n    5300 \n  \n\n Gentoo \n    0 \n    45.1 \n    14.4 \n    210 \n    4400 \n  \n\n Gentoo \n    0 \n    50.1 \n    15.0 \n    225 \n    5000 \n  \n\n Gentoo \n    0 \n    46.5 \n    14.4 \n    217 \n    4900 \n  \n\n Gentoo \n    0 \n    45.0 \n    15.4 \n    220 \n    5050 \n  \n\n Gentoo \n    0 \n    43.8 \n    13.9 \n    208 \n    4300 \n  \n\n Gentoo \n    0 \n    45.5 \n    15.0 \n    220 \n    5000 \n  \n\n Gentoo \n    0 \n    43.2 \n    14.5 \n    208 \n    4450 \n  \n\n Gentoo \n    0 \n    50.4 \n    15.3 \n    224 \n    5550 \n  \n\n Gentoo \n    0 \n    45.3 \n    13.8 \n    208 \n    4200 \n  \n\n Gentoo \n    0 \n    46.2 \n    14.9 \n    221 \n    5300 \n  \n\n Gentoo \n    0 \n    45.7 \n    13.9 \n    214 \n    4400 \n  \n\n Gentoo \n    0 \n    54.3 \n    15.7 \n    231 \n    5650 \n  \n\n Gentoo \n    0 \n    45.8 \n    14.2 \n    219 \n    4700 \n  \n\n Gentoo \n    0 \n    49.8 \n    16.8 \n    230 \n    5700 \n  \n\n Gentoo \n    0 \n    46.2 \n    14.4 \n    214 \n    4650 \n  \n\n Gentoo \n    0 \n    49.5 \n    16.2 \n    229 \n    5800 \n  \n\n Gentoo \n    0 \n    43.5 \n    14.2 \n    220 \n    4700 \n  \n\n Gentoo \n    0 \n    50.7 \n    15.0 \n    223 \n    5550 \n  \n\n Gentoo \n    0 \n    47.7 \n    15.0 \n    216 \n    4750 \n  \n\n Gentoo \n    0 \n    46.4 \n    15.6 \n    221 \n    5000 \n  \n\n Gentoo \n    0 \n    48.2 \n    15.6 \n    221 \n    5100 \n  \n\n Gentoo \n    0 \n    46.5 \n    14.8 \n    217 \n    5200 \n  \n\n Gentoo \n    0 \n    46.4 \n    15.0 \n    216 \n    4700 \n  \n\n Gentoo \n    0 \n    48.6 \n    16.0 \n    230 \n    5800 \n  \n\n Gentoo \n    0 \n    47.5 \n    14.2 \n    209 \n    4600 \n  \n\n Gentoo \n    0 \n    51.1 \n    16.3 \n    220 \n    6000 \n  \n\n Gentoo \n    0 \n    45.2 \n    13.8 \n    215 \n    4750 \n  \n\n Gentoo \n    0 \n    45.2 \n    16.4 \n    223 \n    5950 \n  \n\n Gentoo \n    0 \n    49.1 \n    14.5 \n    212 \n    4625 \n  \n\n Gentoo \n    0 \n    52.5 \n    15.6 \n    221 \n    5450 \n  \n\n Gentoo \n    0 \n    47.4 \n    14.6 \n    212 \n    4725 \n  \n\n Gentoo \n    0 \n    50.0 \n    15.9 \n    224 \n    5350 \n  \n\n Gentoo \n    0 \n    44.9 \n    13.8 \n    212 \n    4750 \n  \n\n Gentoo \n    0 \n    50.8 \n    17.3 \n    228 \n    5600 \n  \n\n Gentoo \n    0 \n    43.4 \n    14.4 \n    218 \n    4600 \n  \n\n Gentoo \n    0 \n    51.3 \n    14.2 \n    218 \n    5300 \n  \n\n Gentoo \n    0 \n    47.5 \n    14.0 \n    212 \n    4875 \n  \n\n Gentoo \n    0 \n    52.1 \n    17.0 \n    230 \n    5550 \n  \n\n Gentoo \n    0 \n    47.5 \n    15.0 \n    218 \n    4950 \n  \n\n Gentoo \n    0 \n    52.2 \n    17.1 \n    228 \n    5400 \n  \n\n Gentoo \n    0 \n    45.5 \n    14.5 \n    212 \n    4750 \n  \n\n Gentoo \n    0 \n    49.5 \n    16.1 \n    224 \n    5650 \n  \n\n Gentoo \n    0 \n    44.5 \n    14.7 \n    214 \n    4850 \n  \n\n Gentoo \n    0 \n    50.8 \n    15.7 \n    226 \n    5200 \n  \n\n Gentoo \n    0 \n    49.4 \n    15.8 \n    216 \n    4925 \n  \n\n Gentoo \n    0 \n    46.9 \n    14.6 \n    222 \n    4875 \n  \n\n Gentoo \n    0 \n    48.4 \n    14.4 \n    203 \n    4625 \n  \n\n Gentoo \n    0 \n    51.1 \n    16.5 \n    225 \n    5250 \n  \n\n Gentoo \n    0 \n    48.5 \n    15.0 \n    219 \n    4850 \n  \n\n Gentoo \n    0 \n    55.9 \n    17.0 \n    228 \n    5600 \n  \n\n Gentoo \n    0 \n    47.2 \n    15.5 \n    215 \n    4975 \n  \n\n Gentoo \n    0 \n    49.1 \n    15.0 \n    228 \n    5500 \n  \n\n Gentoo \n    0 \n    47.3 \n    13.8 \n    216 \n    4725 \n  \n\n Gentoo \n    0 \n    46.8 \n    16.1 \n    215 \n    5500 \n  \n\n Gentoo \n    0 \n    41.7 \n    14.7 \n    210 \n    4700 \n  \n\n Gentoo \n    0 \n    53.4 \n    15.8 \n    219 \n    5500 \n  \n\n Gentoo \n    0 \n    43.3 \n    14.0 \n    208 \n    4575 \n  \n\n Gentoo \n    0 \n    48.1 \n    15.1 \n    209 \n    5500 \n  \n\n Gentoo \n    0 \n    50.5 \n    15.2 \n    216 \n    5000 \n  \n\n Gentoo \n    0 \n    49.8 \n    15.9 \n    229 \n    5950 \n  \n\n Gentoo \n    0 \n    43.5 \n    15.2 \n    213 \n    4650 \n  \n\n Gentoo \n    0 \n    51.5 \n    16.3 \n    230 \n    5500 \n  \n\n Gentoo \n    0 \n    46.2 \n    14.1 \n    217 \n    4375 \n  \n\n Gentoo \n    0 \n    55.1 \n    16.0 \n    230 \n    5850 \n  \n\n Gentoo \n    0 \n    44.5 \n    15.7 \n    217 \n    4875 \n  \n\n Gentoo \n    0 \n    48.8 \n    16.2 \n    222 \n    6000 \n  \n\n Gentoo \n    0 \n    47.2 \n    13.7 \n    214 \n    4925 \n  \n\n Gentoo \n    0 \n    46.8 \n    14.3 \n    215 \n    4850 \n  \n\n Gentoo \n    0 \n    50.4 \n    15.7 \n    222 \n    5750 \n  \n\n Gentoo \n    0 \n    45.2 \n    14.8 \n    212 \n    5200 \n  \n\n Gentoo \n    0 \n    49.9 \n    16.1 \n    213 \n    5400 \n  \n\n\n\n\n\nThe Hmisc::describe() function can give us a quick summary of the data.\n\n\n\n\n\n\nExpand to view detailed summary statistics for each variable\n\n\n\n\n\n\nCodehtml(describe(df))\n\n\n\n\ndf Descriptives\ndf  6  Variables   274  Observations \nspecies \n\n\nn\nmissing\ndistinct\n\n\n274\n0\n2\n\n\n Value      Adelie Gentoo\n Frequency     151    123\n Proportion  0.551  0.449\n \n \nadelie \n\n\nn\nmissing\ndistinct\nInfo\nSum\nMean\nGmd\n\n\n274\n0\n2\n0.742\n151\n0.5511\n0.4966\n\n\n\nbill_length_mm \n\n\nn\nmissing\ndistinct\nInfo\nMean\nGmd\n.05\n.10\n.25\n.50\n.75\n.90\n.95\n\n\n274\n0\n146\n1\n42.7\n5.944\n35.43\n36.20\n38.35\n42.00\n46.68\n49.80\n50.73\n\n\nlowest : 32.1 33.1 33.5 34.0 34.1 ,  highest: 53.4 54.3 55.1 55.9 59.6 \nbill_depth_mm \n\n\nn\nmissing\ndistinct\nInfo\nMean\nGmd\n.05\n.10\n.25\n.50\n.75\n.90\n.95\n\n\n274\n0\n78\n1\n16.84\n2.317\n13.80\n14.20\n15.00\n17.00\n18.50\n19.30\n20.03\n\n\nlowest : 13.1 13.2 13.3 13.4 13.5 ,  highest: 20.6 20.7 21.1 21.2 21.5 \nflipper_length_mm \n\n\nn\nmissing\ndistinct\nInfo\nMean\nGmd\n.05\n.10\n.25\n.50\n.75\n.90\n.95\n\n\n274\n0\n54\n0.999\n202.2\n17.23\n181.0\n184.0\n190.0\n198.0\n215.0\n222.0\n226.7\n\n\nlowest : 172 174 176 178 179 ,  highest: 226 228 229 230 231 \nbody_mass_g \n\n\nn\nmissing\ndistinct\nInfo\nMean\nGmd\n.05\n.10\n.25\n.50\n.75\n.90\n.95\n\n\n274\n0\n89\n1\n4318\n962.3\n3091\n3282\n3600\n4262\n4950\n5535\n5700\n\n\nlowest : 2850 2900 2925 2975 3000 ,  highest: 5850 5950 6000 6050 6300 \n\n\n\n\n\nThe below plot informs us that Adelie and Gentoo penguins are likely to be easily distinguishable based on the measured features, since there is little overlap between the two species. Because we want to have a bit of a challenge (and because logistic regression doesn’t converge if the classes are perfectly separable), we will predict species based on bill length and body mass.\n\nCodedf %>%\n  GGally::ggpairs(mapping = aes(color=species),\n                  columns = c(\"bill_length_mm\",\n                              \"bill_depth_mm\",\n                              \"flipper_length_mm\",\n                              \"body_mass_g\"),\n                  title = \"Can these features distinguish Adelie and Gentoo penguins?\") +\n  scale_color_brewer(palette=\"Dark2\") +\n  scale_fill_brewer(palette=\"Dark2\")\n\n\n\n\nIn order to help our algorithms converge, we will put our variables on a more common scale by converting bill length to cm and body mass to kg.\n\nCodedf$bill_length_cm <- df$bill_length_mm / 10\ndf$body_mass_kg <- df$body_mass_g / 1000\n\n\n\nCode# Look at distribution of bill length in cm and body mass in kg\nqplot(df$bill_length_cm,bins=50) + qplot(df$body_mass_kg, bins=50)"
  },
  {
    "objectID": "posts/odds-ratio-dichotomize/index.html",
    "href": "posts/odds-ratio-dichotomize/index.html",
    "title": "Sensitivity of odds-ratios calculated on dichotomized variables to inclusion criteria",
    "section": "",
    "text": "Codelibrary(tidyverse)\nset.seed(123)"
  },
  {
    "objectID": "posts/odds-ratio-dichotomize/index.html#motivation",
    "href": "posts/odds-ratio-dichotomize/index.html#motivation",
    "title": "Sensitivity of odds-ratios calculated on dichotomized variables to inclusion criteria",
    "section": "Motivation",
    "text": "Motivation\nIn this document, we will show how calculating an odds ratio based on a dichotomized continuous predictor variable can be manipulated by changing the range of the predictor variable that was sampled (i.e, study inclusion criteria), whereas a logistic regression model that uses the continuous values of the predictor will produce a stable estimate."
  },
  {
    "objectID": "posts/odds-ratio-dichotomize/index.html#scenario",
    "href": "posts/odds-ratio-dichotomize/index.html#scenario",
    "title": "Sensitivity of odds-ratios calculated on dichotomized variables to inclusion criteria",
    "section": "Scenario",
    "text": "Scenario\nAssume that we are interested in a disease where the incidence varies with age.\nWe will assume as the true model a simple relationship where the probability of developing the disease is a linear function of age. The below plot shows this relationship.\n\nCode# True model of disease probability is a linear function of age\np_disease <- function(age){\n  0.25 + 0.0075*age\n}\n\n# Plot true model\ntibble(age = seq(20, 80, length.out=2), prob = p_disease(age)) %>%\n  ggplot() +\n  aes(x=age, y=prob) +\n  geom_line() +\n  labs(title = \"True probability of having the disease\",\n       x=\"Age\",\n       y=\"P(Disease)\") +\n  theme_bw()\n\n\n\n\nWe decide to sample subjects from the population and record if they have the disease. For simplicity, assume we sample patients uniformly within a given age range. We will show that dichotomizing age at a cutpoint is not a good idea, and can lead to estimates that can be greatly affected by the chosen age range to be sampled.\nTo dichotomize the predictor variable, let’s compare the incidence of disease among old (age > 50) and young (age < 50) patients and calculate an odds ratio, instead of using age as a continuous variable. The below simulation shows the results of two scenarios. As a comparison, we also fit a logistic regression using continuous age.\nFirst, we sample 10,000 subjects with ages between 40 and 60. Second, we sample 10,000 subjects with ages between 20 and 80. We show that the choices of inclusion criteria has a large effect on the odds ratio comparing odds of disease between young and old subjects, but the estimates provided by logistic regression are unchanged."
  },
  {
    "objectID": "posts/odds-ratio-dichotomize/index.html#simulation",
    "href": "posts/odds-ratio-dichotomize/index.html#simulation",
    "title": "Sensitivity of odds-ratios calculated on dichotomized variables to inclusion criteria",
    "section": "Simulation",
    "text": "Simulation\nSample from ages 40 to 60\n\nCode# Draw 10,000 patients uniformly between 40 and 60\nages <- runif(10000, min=40, max=60)\n\n# Calculate true probabilities for each patient\nprobs <- p_disease(ages)\n\n# Generate data where each patient has `probs` probability of having the disease\ndata <- map_dbl(probs, ~sample(c(0,1), size=1, prob=c(1-.x, .x)))\n\n\n\nCode# Put simulation data into a data frame\ndf <- tibble(age=ages, prob=probs, disease=data)\n\n# Dichotomize at age = 50\ndf$old <- (df$age > 50)\n\nhead(df)\n\n\n\n\nage\nprob\ndisease\nold\n\n\n\n45.75155\n0.5931366\n1\nFALSE\n\n\n55.76610\n0.6682458\n1\nTRUE\n\n\n48.17954\n0.6113465\n0\nFALSE\n\n\n57.66035\n0.6824526\n1\nTRUE\n\n\n58.80935\n0.6910701\n1\nTRUE\n\n\n40.91113\n0.5568335\n1\nFALSE\n\n\n\n\n\n\n\nCodetable(df$disease, df$old)\n\n   \n    FALSE TRUE\n  0  2081 1614\n  1  2976 3329\n\n\n\nCodeodds_ratio <- (3329 / 1614) / (2976 / 2081)\n\nodds_ratio\n\n[1] 1.442279\n\n\n\nCode# Fit logistic regression model using continuous age\nglm(disease ~ age, family=binomial(), data=df)\n\n\nCall:  glm(formula = disease ~ age, family = binomial(), data = df)\n\nCoefficients:\n(Intercept)          age  \n   -1.23211      0.03547  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9998 Residual\nNull Deviance:      13170 \nResidual Deviance: 13080    AIC: 13080\n\n\nSample from ages 20 to 80\n\nCodeages <- runif(10000, min=20, max=80)\nprobs <- p_disease(ages)\ndata <- map_dbl(probs, ~sample(c(0,1), size=1, prob=c(1-.x, .x)))\ndf <- tibble(age=ages, prob=probs, disease=data)\ndf$old <- (df$age > 50)\n\ntable(df$disease, df$old)\n\n   \n    FALSE TRUE\n  0  2400 1179\n  1  2625 3796\n\n\n\nCodeodds_ratio <- (3796 / 1179) / (2625 / 2400)\n\nodds_ratio\n\n[1] 2.943705\n\n\n\nCodeglm(disease ~ age, family=binomial(), data=df)\n\n\nCall:  glm(formula = disease ~ age, family = binomial(), data = df)\n\nCoefficients:\n(Intercept)          age  \n   -1.15573      0.03586  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9998 Residual\nNull Deviance:      13040 \nResidual Deviance: 12220    AIC: 12230"
  },
  {
    "objectID": "posts/odds-ratio-dichotomize/index.html#conclusion",
    "href": "posts/odds-ratio-dichotomize/index.html#conclusion",
    "title": "Sensitivity of odds-ratios calculated on dichotomized variables to inclusion criteria",
    "section": "Conclusion",
    "text": "Conclusion\nWe see that when sampling from ages 40 to 60, the dichotomization approach estimated an odds ratio of 1.44 compared to an odds ratio of 2.94 when sampling from ages 20 to 80.\nIn contrast, when sampling from ages 40 to 60, the logistic regression estimated a regression coefficient for age of 0.0355 compared to a very similar value of 0.0359 when sampling from ages 20 to 80."
  },
  {
    "objectID": "posts/robust-estimators/index.html",
    "href": "posts/robust-estimators/index.html",
    "title": "Statistical simulation of robust estimators with tidyverse tools",
    "section": "",
    "text": "Code# Load packages\nlibrary(tidyverse)\n\n# Formatting of HTML tables\nlibrary(kableExtra)\n\n# Set global ggplot theme\ntheme_set(cowplot::theme_cowplot(font_size=12,\n                                 font_family = \"Source Sans Pro\"))\n\nset.seed(7)\nThe sample mean and sample median are commonly used estimators for the center of distribution. There is no such thing as a “best estimator” in all circumstances. However, estimators can outperform other estimators in terms of desirable properties (e.g., unbiasedness, low variance, consistency) given a particular circumstance. We can use simulation and mathematical theory to evaluate the performance of estimators. Here we focus on using simulation, with the help of tools from the tidyverse."
  },
  {
    "objectID": "posts/robust-estimators/index.html#scenario-1",
    "href": "posts/robust-estimators/index.html#scenario-1",
    "title": "Statistical simulation of robust estimators with tidyverse tools",
    "section": "Scenario 1",
    "text": "Scenario 1\nWe begin with a simple example. The true data generating process is \\[ X_1, X_2, \\ldots X_n \\stackrel{iid}{\\sim} N(3,1) \\]\nHow will the mean and median perform as estimators of the true mean, \\(\\mu = 3\\)? Let’s use simulation to find out.\nWe will use a tibble to store all of our simulation results. First, let’s decide what sample sizes to simulate, and how many trials to run. The more trials we run, the more accurate our simulation results will be – the cost being increased time to run the simulations and memory to store the results.\n\nCodesize <- c(5,10,20,50,100,200)\ntrial <- 1:1e5\n\n\nNow we use crossing() to generate a tibble that contains every combination of the vectors size and trial. So for every sample size, we are repeating it 100,000 times.\n\nCodedf <- crossing(trial, size)\n\n\nWe can look at the first 15 rows:\n\n\n# A tibble: 10 × 2\n   trial  size\n   <int> <dbl>\n 1     1     5\n 2     1    10\n 3     1    20\n 4     1    50\n 5     1   100\n 6     1   200\n 7     2     5\n 8     2    10\n 9     2    20\n10     2    50\n\n\nNow for each row, we want to add to our data frame a sample of data with the sample size given by that row. We will use purrr::map() to do this.\n\nCodedf$data <- map(df$size, ~rnorm(n=.x, mean = 3, sd=1))\n\n\nThe first argument to map is the vector to iterate over, and the second argument is the function to apply. We use .x as a dummy variable to refer to the value in the current iteration. We can interpret this as saying, for each of the \\(6 \\times 10^5\\) size records in our data, generate size observations from a N(3,1) distribution.\nThe new column, data, is a list of lists, where each list contains a sample of data. Let’s see what this looks like.\n\n\n# A tibble: 10 × 3\n   trial  size data       \n   <int> <dbl> <list>     \n 1     1     5 <dbl [5]>  \n 2     1    10 <dbl [10]> \n 3     1    20 <dbl [20]> \n 4     1    50 <dbl [50]> \n 5     1   100 <dbl [100]>\n 6     1   200 <dbl [200]>\n 7     2     5 <dbl [5]>  \n 8     2    10 <dbl [10]> \n 9     2    20 <dbl [20]> \n10     2    50 <dbl [50]> \n\n\nNow that we have our data, we can compute the mean and median for each sample.\n\nCodedf$mean <- map_dbl(df$data, ~mean(.x))\ndf$median <- map_dbl(df$data, ~median(.x))\n\n\n\n\n# A tibble: 10 × 5\n   trial  size data         mean median\n   <int> <dbl> <list>      <dbl>  <dbl>\n 1     1     5 <dbl [5]>    2.80   2.31\n 2     1    10 <dbl [10]>   3.96   3.55\n 3     1    20 <dbl [20]>   3.14   3.15\n 4     1    50 <dbl [50]>   3.07   3.09\n 5     1   100 <dbl [100]>  3.16   3.32\n 6     1   200 <dbl [200]>  2.96   2.95\n 7     2     5 <dbl [5]>    2.52   2.78\n 8     2    10 <dbl [10]>   2.83   2.80\n 9     2    20 <dbl [20]>   3.08   3.14\n10     2    50 <dbl [50]>   2.93   2.86\n\n\nThe mean and median of each sample are now in separate columns. However, to get the data into tidy format, also known as long format, we want them in separate rows. Having the data in tidy format allows us to use ggplot2 and other tidyverse functions more effectively. We use pivot_longer to do this.\n\nCodedf <- pivot_longer(df,\n                   cols=mean:median,\n                   names_to=\"Estimator\",\n                   values_to=\"Estimate\")\n\n\n\n\n# A tibble: 10 × 5\n   trial  size data        Estimator Estimate\n   <int> <dbl> <list>      <chr>        <dbl>\n 1     1     5 <dbl [5]>   mean          2.80\n 2     1     5 <dbl [5]>   median        2.31\n 3     1    10 <dbl [10]>  mean          3.96\n 4     1    10 <dbl [10]>  median        3.55\n 5     1    20 <dbl [20]>  mean          3.14\n 6     1    20 <dbl [20]>  median        3.15\n 7     1    50 <dbl [50]>  mean          3.07\n 8     1    50 <dbl [50]>  median        3.09\n 9     1   100 <dbl [100]> mean          3.16\n10     1   100 <dbl [100]> median        3.32\n\n\nNow we are finally ready to analyze the results of our simulation. First, let’s compute the bias and variance of our estimators for each sample size.\n\nCodesummary_df <-\ndf %>%\n  group_by(size, Estimator) %>%\n  summarize(Bias = (mean(Estimate) - 3),\n            Variance = var(Estimate)) %>%\n  pivot_longer(Bias:Variance)\n\n\n\n\n# A tibble: 24 × 4\n# Groups:   size [6]\n    size Estimator name          value\n   <dbl> <chr>     <chr>         <dbl>\n 1     5 mean      Bias      0.0000212\n 2     5 mean      Variance  0.199    \n 3     5 median    Bias      0.00141  \n 4     5 median    Variance  0.286    \n 5    10 mean      Bias     -0.000959 \n 6    10 mean      Variance  0.100    \n 7    10 median    Bias     -0.000314 \n 8    10 median    Variance  0.138    \n 9    20 mean      Bias      0.000634 \n10    20 mean      Variance  0.0502   \n# … with 14 more rows\n\n\nPlotting the bias and variance as a function of sample size, we see that both the mean and median are unbiased estimators of the center of the true distribution, but the median has higher variance. Therefore, we would prefer the mean under these assumptions.\n\nCodesummary_df %>%\n  ggplot() +\n  aes(x=size, y=value, color=Estimator) +\n  geom_line(alpha=0.6) +\n  geom_point(alpha=0.6) +\n  facet_wrap(~name) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.8,0.8)) +\n  labs(\n    x = \"Sample Size\",\n    y = \"Estimated Value\"\n  )\n\n\n\n\nPlotting the sampling distribution for each of the estimators shows that the median indeed has higher variance.\n\nCodelabel_names <- as_labeller(c(`5` = \"Sample Size: 5\",\n                             `10` = \"Sample Size: 10\",\n                             `20` = \"Sample Size: 20\",\n                             `50` = \"Sample Size: 50\",\n                             `100` = \"Sample Size: 100\",\n                             `200` = \"Sample Size: 200\")) \n\ndf %>%\n  ggplot() +\n  aes(x=Estimate, color=Estimator, fill=Estimator) +\n  geom_density(alpha=0.3, size=0.8) +\n  facet_wrap(~size, labeller=label_names) +\n  geom_vline(aes(xintercept = 3), linetype=2, alpha=0.3) +\n  coord_cartesian(xlim=c(1,5), ylim=c(0,6)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.88,0.88)) +\n  labs(\n    title = \"Normal(3,1) Distribution\",\n    subtitle = \"Performance of mean and median\",\n    x=\"Estimate\",\n    y=\"PDF\"\n  )"
  },
  {
    "objectID": "posts/robust-estimators/index.html#scenario-2",
    "href": "posts/robust-estimators/index.html#scenario-2",
    "title": "Statistical simulation of robust estimators with tidyverse tools",
    "section": "Scenario 2",
    "text": "Scenario 2\nNow let’s take a look at a distribution with heavier tails than the normal. An example is a mixture of normal two distributions.\nThe data-generating process is this:\n\nWith probability 0.9, draw from the \\(N(3, 1)\\) distribution.\nOtherwise, (with probability 0.1), draw from the \\(N(3, 10)\\) distribution.\n\nWe can write a function to draw from this distribution.\n\nCode# generates 1 draw from the specifies mixture normal distribution\nmixed_normal <- function(){\n  x <- runif(1)\n  if (x>0.1) {\n    return(rnorm(n=1, mean = 3, sd=1))\n  }\n  else{\n    return(rnorm(n=1, mean = 3, sd=10))\n  }\n}\n\n# generates n draws from the specifies mixture normal distribution\nrmixed_norm  <- function(n){\n  map_dbl(1:n, ~mixed_normal())\n}\n\n\nPlotting the normal distribution and the mixture distribution on top of each other, we see that they are very similar, but the mixture distribution has heavier tails (i.e., more of the probability mass is in the tails compared to the normal distribution).\n\nCodetibble(normal = rnorm(1e5, mean=3),\n       mixture = rmixed_norm(1e5)) %>%\n  pivot_longer(cols=normal:mixture, names_to=\"Distribution\", values_to=\"value\") %>%\n  ggplot() +\n  aes(x=value, color=Distribution) +\n  geom_density(alpha=0.7)+\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.88,0.90)) +\n  labs(\n    title = \"Mixture Normal vs Normal Distribution\",\n    subtitle = \"\",\n    x= \"X\",\n    y= \"PDF\"\n  )\n\n\n\n\nNow let’s compare the performance of the mean and median on the mixture distribution.\n\nCodesize <- c(5,10,20,50,100,200)\ntrial <- 1:1e5\n\ndf <- crossing(trial, size)\n\ndf$data <- map(df$size, ~rmixed_norm(n=.x))\n\ndf$mean <- map_dbl(df$data, ~mean(.x))\ndf$median <- map_dbl(df$data, ~median(.x))\n\ndf <- pivot_longer(df, cols=mean:median, names_to=\"Estimator\", values_to=\"Estimate\")\n\ndf %>%\n  group_by(size, Estimator) %>%\n  summarize(Bias = (mean(Estimate) - 3),\n            Variance = var(Estimate)) %>%\n  pivot_longer(Bias:Variance) -> summary_df\n\n\nsummary_df %>%\n  ggplot() +\n  aes(x=size, y=value, color=Estimator) +\n  geom_line(alpha=0.6) +\n  geom_point(alpha=0.6) +\n  facet_wrap(~name) +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.8,0.8)) +\n  labs(\n    x = \"Sample Size\",\n    y = \"Estimated Value\"\n  )\n\n\n\nCodelabel_names <- as_labeller(c(`5` = \"Sample Size: 5\",\n                             `10` = \"Sample Size: 10\",\n                             `20` = \"Sample Size: 20\",\n                             `50` = \"Sample Size: 50\",\n                             `100` = \"Sample Size: 100\",\n                             `200` = \"Sample Size: 200\")) \n\ndf %>%\n  ggplot() +\n  aes(x=Estimate, color=Estimator, fill=Estimator) +\n  geom_density(alpha=0.3, size=0.8) +\n  facet_wrap(~size, labeller=label_names) +\n  geom_vline(aes(xintercept = 3), linetype=2, alpha=0.3) +\n  coord_cartesian(xlim=c(1,5), ylim=c(0,6)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(legend.position = c(0.88,0.88)) +\n  labs(\n    title = \"Mixture Normal Distribution\",\n    subtitle = \"Performance of mean and median\",\n    x=\"Estimate\",\n    y=\"PDF\"\n  )\n\n\n\n\nIt looks like the median greatly outperforms the mean! Both are unbiased, but the median has lower variance."
  },
  {
    "objectID": "posts/robust-estimators/index.html#take-away-points",
    "href": "posts/robust-estimators/index.html#take-away-points",
    "title": "Statistical simulation of robust estimators with tidyverse tools",
    "section": "Take-away points",
    "text": "Take-away points\n\nSimulation is a powerful tool in statistics. Here we showed how is can be used to compare the properties of estimators.\nFor distributions with heavy tails, the median may be a better estimator of center than the mean."
  },
  {
    "objectID": "posts/tmerge/index.html",
    "href": "posts/tmerge/index.html",
    "title": "Using the tmerge() function to structure time-dependent covariates for survival analysis",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(survival)\nlibrary(kableExtra)"
  },
  {
    "objectID": "posts/tmerge/index.html#why-do-we-need-tmerge",
    "href": "posts/tmerge/index.html#why-do-we-need-tmerge",
    "title": "Using the tmerge() function to structure time-dependent covariates for survival analysis",
    "section": "Why do we need tmerge?",
    "text": "Why do we need tmerge?\nIn survival analysis, we differentiate between time-independent covariates and time-dependent covariates. Time-independent covariates are constant over time, while time-dependent covariates can vary over time.\nAs an example, assume we are modeling time-to-death in years, with exposure to a chemical as our time-dependent covariate of interest. Assume that we can quantify the exposure as 0, 1, or 2. We will use this exposure and the sex of the subject as covariates in our model.\nTo represent time-dependent covariates, we need to have multiple rows for each subject, where each row represents a different value of the time dependent covariates.\nTake for example a subject who started with exposure = 1. Then at 4 years, their exposure status changed to exposure = 0. Then at 7 years, their exposure status changed to exposure = 2. Then the subject died at 10 years (i.e., status = 1). We would need three rows to represent this subject, since there are 3 distinct time periods: 0-4, 4-7, and 7-10. The survival package uses the names tstart and tstop to denote the beginning and end of each time period. So when structuring data from time-dependent variables, the rows for this subject would look like this:\n\n\n\n\n\n id \n    tstart \n    tstop \n    exposure \n    status \n  \n\n\n 1 \n    0 \n    4 \n    1 \n    0 \n  \n\n 1 \n    4 \n    7 \n    0 \n    0 \n  \n\n 1 \n    7 \n    10 \n    2 \n    1"
  },
  {
    "objectID": "posts/tmerge/index.html#using-tmerge",
    "href": "posts/tmerge/index.html#using-tmerge",
    "title": "Using the tmerge() function to structure time-dependent covariates for survival analysis",
    "section": "Using tmerge\n",
    "text": "Using tmerge\n\nCreating example data\nFirst, we need our data in two data frames, one for the time-independent covariates and one for the time-dependent covariates.\nHere’s some example data for the time-independent covariates. We have 3 subjects, and each row contains their id, sex, survival time, and whether or not they experience the event of interest (in this case, death). We use event = 1 to indicate death, and event = 0 to indicate censoring.\n\nCodedf_time_ind <-\n  tibble(id = c(1,2,3),\n         sex = c(\"M\",\"F\",\"F\"),\n         surv_time = c(5,10,15),\n         event = c(1,1,0))\n\n\n\n\n\n\n\n id \n    sex \n    surv_time \n    event \n  \n\n\n 1 \n    M \n    5 \n    1 \n  \n\n 2 \n    F \n    10 \n    1 \n  \n\n 3 \n    F \n    15 \n    0 \n  \n\n\n\n\nAnd here’s some example time-dependent data. Each subject has a record for their exposure status at time = 0, and another record whenever their exposure status changes. For example, in the data below, subject 1 has\n\nexposure status 0 from time 0 to 2\nexposure status 1 from time 2 to 4\nexposure status 2 from time 4 onwards\n\n\nCodedf_time_dep <-\n  tibble(id = c(1,1,1,2,2,3),\n         time = c(0,2,4,0,7,0),\n         exposure = c(0,1,2,0,1,0))\n\n\n\n\n\n\n\n id \n    time \n    exposure \n  \n\n\n 1 \n    0 \n    0 \n  \n\n 1 \n    2 \n    1 \n  \n\n 1 \n    4 \n    2 \n  \n\n 2 \n    0 \n    0 \n  \n\n 2 \n    7 \n    1 \n  \n\n 3 \n    0 \n    0 \n  \n\n\n\n\nWe will use the tmerge function to turn these data frames in a single data frame to use in a time-dependent survival analysis. The tmerge function is used multiple times in the process of formatting data for time-dependent covariates.\nFirst, we use tmerge with the independent variables. Note that we call tmerge with df_time_ind as both the data1 and data2 argument. We must also specify the id variable and the event variable using the syntax event(survival_time_variable, event_indicator_variable). Using the name event on the left of the expression is optional.\n\nCodedf_time_ind <-\n  tmerge(data1=df_time_ind,\n         data2=df_time_ind,\n         id=id,\n         event=event(surv_time, event))\n\n\nNow the df_time_ind data frame looks like this:\n\n\n\n\n\n id \n    sex \n    surv_time \n    event \n    tstart \n    tstop \n  \n\n\n 1 \n    M \n    5 \n    1 \n    0 \n    5 \n  \n\n 2 \n    F \n    10 \n    1 \n    0 \n    10 \n  \n\n 3 \n    F \n    15 \n    0 \n    0 \n    15 \n  \n\n\n\n\nNotice that the tstart, tstart, and event variables have been added.\nNow to add the time-dependent variables, we call tmerge again, now with df_time_ind as the data1 argument and df_time_dep as the data2 argument. To specify the time-dependent exposure variable, we use the tdc function with the syntax time_dependent_variable = tdc(time, time_dependent_variable).\n\nCodedf_final <-\ntmerge(data1=df_time_ind,\n       data2=df_time_dep,\n       id=id,\n       exposure=tdc(time, exposure))\n\n\nBelow we have our completed dataset with properly structured time-dependent variables.\n\n\n\n\n\n id \n    sex \n    surv_time \n    event \n    tstart \n    tstop \n    exposure \n  \n\n\n 1 \n    M \n    5 \n    0 \n    0 \n    2 \n    0 \n  \n\n 1 \n    M \n    5 \n    0 \n    2 \n    4 \n    1 \n  \n\n 1 \n    M \n    5 \n    1 \n    4 \n    5 \n    2 \n  \n\n 2 \n    F \n    10 \n    0 \n    0 \n    7 \n    0 \n  \n\n 2 \n    F \n    10 \n    1 \n    7 \n    10 \n    1 \n  \n\n 3 \n    F \n    15 \n    0 \n    0 \n    15 \n    0 \n  \n\n\n\n\nFinally, fitting a model with the survival package uses the general syntax Surv(tstart, tstop, event_indicator_variable) as shown below, where we fit a Cox proportional hazard model.\n\nCodecoxph(Surv(tstart, tstop, event) ~ exposure, data=df_final)"
  },
  {
    "objectID": "posts/tmerge/index.html#references",
    "href": "posts/tmerge/index.html#references",
    "title": "Using the tmerge() function to structure time-dependent covariates for survival analysis",
    "section": "References",
    "text": "References\nFor more details, see this presentation and this report on further features of tmerge."
  },
  {
    "objectID": "posts/what-is-a-statistic/index.html",
    "href": "posts/what-is-a-statistic/index.html",
    "title": "What is a statistic?",
    "section": "",
    "text": "Codelibrary(tidyverse)"
  },
  {
    "objectID": "posts/what-is-a-statistic/index.html#introduction",
    "href": "posts/what-is-a-statistic/index.html#introduction",
    "title": "What is a statistic?",
    "section": "Introduction",
    "text": "Introduction\nWhen we collect data from a data-generating process, we can calculate values from that data. These values are called statistics.\nCommon example include:\n\nmean and median (measures of center)\nvariance and IQR (measures of spread)\norder statistics, such as the minimum and the maximum\n\nWe can even create arbitrary statistics that appear to have little use, such as adding only the first and third elements of the data and dividing by 17."
  },
  {
    "objectID": "posts/what-is-a-statistic/index.html#simulating-statistics-of-dice-rolls",
    "href": "posts/what-is-a-statistic/index.html#simulating-statistics-of-dice-rolls",
    "title": "What is a statistic?",
    "section": "Simulating statistics of dice rolls",
    "text": "Simulating statistics of dice rolls\nAs a simple data-generating process, let’s consider rolling 5 dice. Each time we roll, we obtain 5 numbers, each from 1 to 6. We will call each one of these vectors of 5 numbers,\n\\[\n(x_1, x_2, x_3, x_4, x_5)\n\\]\na sample. We then will compute statistics from these samples. The main question we seek to answer is: how are the statistics distributed? When I calculate the mean of 5 dice, what will the most likely result be? We can ask this question about any statistic.\nWe’ll write a function to roll n dice called roll().\n\nCode# A function to roll `n` dice\nroll <- function(n){\n  sample(x = 1:6, size=n, replace=TRUE)\n}\n\n\nThen we’ll use purrr:map() to generate 100,000 rolls of 5 dice.\n\nCode# Roll 5 dice 100,000 times\ndata <- map(1:1e5, ~roll(5))\n\n\nHere’s an example of running the function.\n\nCode# Look at first three rolls\ndata[1:3]\n\n[[1]]\n[1] 3 1 4 2 5\n\n[[2]]\n[1] 5 3 3 1 1\n\n[[3]]\n[1] 5 3 3 3 2\n\n\nFor each of these rolls, we can calculate the value of a statistic.\nWe’ll calculate the following statistics:\n\nmedian\nmean\nminimum\nmaximum\nsecond order statistic \\(X_{(2)}\\)\n\nrange\n\n\nCode# Returns the nth order statistic of the sample\norder_stat <- function(x, n){\n  x <- sort(x)\n  return(x[n])\n}\n\n# Generate various statistics for each roll\nmedians <- map_dbl(data, ~median(.x))\nmeans <- map_dbl(data, ~mean(.x))\nminimums <- map_dbl(data, ~min(.x))\nmaximums <- map_dbl(data, ~max(.x))\nsecond_order_stat <- map_dbl(data, ~order_stat(x=.x, n=2))\nranges <- maximums - minimums\n\n\n\nCode# Create a data frame from our computed statistics\ndf <- tibble(medians, means, minimums, maximums, second_order_stat, ranges)\n\n# Pivot the data into long format for plotting\ndf <- pivot_longer(df, cols = everything())\n\n\nNow using the data from our simulation, we can plot the sampling distribution of the each of the statistics.\n\nCodedf$name <- recode(df$name,\n  `medians` = \"Median\",\n  `means` = \"Mean\",\n  `minimums` = \"Minimum\",\n  `maximums` = \"Maximum\",\n  `second_order_stat` = \"2nd Order Statistic\",\n  `ranges` = \"Range\")\n\ndf$name <- as.factor(df$name)\ndf$name <- fct_relevel(df$name,\n                       c(\"Minimum\",\n                         \"2nd Order Statistic\",\n                         \"Maximum\",\n                         \"Range\",\n                         \"Mean\",\n                         \"Median\"))\n\ndf %>%\n  ggplot(aes(x = value)) +\n  geom_bar(aes(y = ..prop..),\n           width = 0.2, fill = \"gray\", color = \"black\") +\n  scale_x_continuous(breaks = 0:6) +\n  facet_wrap(~name, scales = \"free_x\") +\n  labs(x = \"Value\",\n       y = \"Estimated Probability\",\n       title = \"Distribution of various statistics for 100,000 rolls of 5 dice\",\n       caption = \"Monte Carlo estimate with 100,000 simulations\") +\n  ggthemes::theme_solarized() +\n  theme(text = element_text(size = 12, family = \"Source Sans Pro\"))\n\n\n\nDistributions of statistics computed from rolls of 5 dice. Probabilities were estimated using 100,000 simulations.\n\n\n\n\nA few things to note:\n\nBecause of averaging, the mean can take on more possible values than the other statistics. Qe can see it taking on the characteristic bell shape of the normal distribution due to the central limit theorem.\nThe median is always a whole number because we are rolling an odd number of dice.\nSome of these distributions are tedious to work out analytically, and with more complicated data-generating processes there may be no closed form solutions."
  },
  {
    "objectID": "quick_notes/stationary_distribution_MCMC/index.html",
    "href": "quick_notes/stationary_distribution_MCMC/index.html",
    "title": "The stationary distribution of a Markov Chain",
    "section": "",
    "text": "Codelibrary(tidyverse)\n\n\n\nCode# Create the transition matrix\nm <- matrix(c(0.8, 0.2,\n              0.6, 0.4),\n            byrow = TRUE,\n            nrow=2,\n            ncol=2)\n\n\n\nCodechain_length <- 1e5\n\n# Sample from the Markov chain\ninitial_state <- 1\nchain <- numeric(chain_length)\nchain[1] <- initial_state\nfor (i in 2:chain_length) {\n  prev_state <- chain[i-1]\n  chain[i] <- sample(c(1,2),\n                     size=1,\n                     prob = c(m[prev_state, 1], m[prev_state ,2]))\n}\n\n\n\nCode# Look at long-run probabilities of being in each state\ntable(chain) / chain_length\n\nchain\n      1       2 \n0.74888 0.25112"
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "R Shiny Apps",
    "section": "",
    "text": "Understanding cumulative ordinal models\nLink: https://maximilian-rohde.shinyapps.io/ordinal_model_shiny/\nPreview:"
  }
]