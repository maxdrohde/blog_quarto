---
title: Logistic regression (by hand)
description: An in-depth dive into the workings of logistic regression.
author: Max Rohde
date: 05/11/2022
image: preview.png
code-fold: show
freeze: false
draft: false

reference-location: margin
citation-location: margin
---

```{r}
# Load packages
library(tidyverse)
library(gganimate)

library(Hmisc)

library(palmerpenguins)

library(patchwork)
library(kableExtra)
library(glue)

# Set global ggplot theme
theme_set(cowplot::theme_cowplot(font_size=14,
                                 font_family = "Source Sans Pro"))
```


# Overview

Logistic regression is a method for estimating the probability that an observation is in one of two classes given a vector of covariates. For example, given various demographic characteristics (age, sex, etc...), we can estimate the probability that a person owns a home or not. Important predictors would likely be age and level of income. The probabilities returned by the model can then be used for classification based on a cutoff (e.g., 0.5 is a common choice) or used for decision making based on more complex decision rules^[This [article](https://www.fharrell.com/post/classification/) goes into more detail on the difference between prediction of probabilities and classification.]. 

In this post, we will explore how logistic regression works by implementing it by hand. It can be bit of a black box when the built-in functions in R carry out the procedure, so even though it will never be practical, implementing algorithms by hand can help with understanding how they work.

# Data

As an example dataset, we will use the Palmer Penguins data. From the help file for the data, the data includes "measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex".

We'll load the data and save it as a data frame `df`.

```{r}
# Load and rename data
data(penguins)
df <- penguins
```

Then we can

- filter to two of the penguin species: Adelie and Gentoo
- create a binary variable, `adelie` corresponding to 1 for Adelie and 0 for Gentoo
- select the columns that we want to keep

```{r}
df <-
df %>%
  filter(species %in% c("Adelie", "Gentoo")) %>%
  mutate(adelie = as.integer(species == "Adelie")) %>%
  select(species, adelie, bill_length_mm:body_mass_g) %>%
  drop_na()
```

## Exploratory data analysis

You can explore the raw data below.

```{r}
#| echo: false

# View data
df %>%
  kbl() %>%
  kable_styling() %>%
  scroll_box(height = "300px")
```


The `Hmisc::describe()` function can give us a quick summary of the data.

:::{.callout-note .column-page-right collapse="true"}
## Expand to view detailed summary statistics for each variable

```{r}
html(describe(df))
```

:::

The below plot informs us that Adelie and Gentoo penguins are likely to be easily distinguishable based on the measured features, since there is little overlap between the two species. Because we want to have a bit of a challenge (and because logistic regression doesn't converge if the classes are perfectly separable), we will predict species based on bill length and body mass.

```{r}
#| column: page-right
#| fig-width: 12
df %>%
  GGally::ggpairs(mapping = aes(color=species),
                  columns = c("bill_length_mm",
                              "bill_depth_mm",
                              "flipper_length_mm",
                              "body_mass_g"),
                  title = "Can these features distinguish Adelie and Gentoo penguins?") +
  scale_color_brewer(palette="Dark2") +
  scale_fill_brewer(palette="Dark2")
```

In order to help our algorithms converge, we will put our variables on a more common scale by converting bill length to cm and body mass to kg.

```{r}
df$bill_length_cm <- df$bill_length_mm / 10
df$body_mass_kg <- df$body_mass_g / 1000
```

# Logistic regression overview

Logistic regression is a type of linear model. In statistics, a linear model means linear in the parameters, so we are modeling the output as a linear function of the parameters.

For example, if we were predicting bill length, we could create a linear model where bill length is normally distributed, with a mean determined as a linear function of body mass and species.

\begin{gather}
\mu = \beta_0 + \beta_1 [\text{Body Mass}] + \beta_2 [\text{Species = Adelie}] \\
\text{[Bill Length]} \sim N(\mu, \sigma^2)
\end{gather}

We have three parameters, $\beta_0$, $\beta_1$, and $\beta_2$. We can determine the likelihood of the data given these parameters. Maximizing the likelihood is the [most common way](https://online.stat.psu.edu/stat415/lesson/1/1.2) to estimate the parameters from data. The idea is that we tune the parameters until we find the set of parameters that made the observed data most likely.

However, this won't quite work if we want to predict a binary outcome like species. We could form a model like this:
\begin{gather}
p = \beta_0 + \beta_1 [\text{Bill Length}] + \beta_2 [\text{Body Mass}] \\
\text{[Species]} \sim \operatorname{Bernoulli}(p)
\end{gather}
In words, each observation of a penguin is modeled as a Bernoulli random variable, where the probability of being Adelie is a linear function of bill length and body mass. The issue with this model is that if we let the parameters vary, the can exceed the range $[0,1]$, which doesn't make sense if we are trying to model a probability.

The solution is the expit function:
$$
\operatorname{expit} = \frac{e^{x}}{1+e^{x}}
$$

This function takes in a real valued input and transforms it to lie within $[0,1]$. This also called the logistic function, hence the name "logistic regression".

Let's try it out in an example.

```{r}
# This is a naive implementation that can overflow for large x
# expit <- function(x) exp(x) / (1 + exp(x))

# Better to use the built-in version
expit <- plogis
```

We see that anything below -5 gets squashed to zero and anything above 5 gets squashed to 1.

```{r}
# Plot the output of the expit() function for x values between -10 and 10
x <- seq(-10, 10, length.out=1e5)
plot(x,
     expit(x),
     type = "l",
     main = "Understanding the effect of expit")
```

We then modify our model to be
\begin{gather}
p_i = \operatorname{expit} \left(\beta_0 + \beta_1 [\text{Bill Length}] + \beta_2 [\text{Body Mass}] \right) \\
\text{[Species]} \sim \operatorname{Bernoulli}(p_i)
\end{gather}
so now $p_i$ is constrained to lie within $[0,1]$. This type of model is known as a generalized linear model (GLM).

The likelihood contribution of a single observation is $p_i$ if it is Adelie and $1-p_i$ if it is Gentoo, which we can write as

$$
\text{Likelihood}_i = p_i^{\text{Adelie}} (1-p_i)^{1 - \text{Adelie}}
$$

::: {.column-margin}
This form of writing the Bernoulli PMF works because if Adelie = 1, then $\text{Likelihood}_i = p_i^{1} (1-p_i)^{1 - 1} = p_i$ and if Adelie = 0, then $\text{Likelihood}_i = p_i^{0} (1-p_i)^{1 - 0} = 1-p_i$.
:::

and therefore the log-likelihood contribution of a single observation is

$$
\text{Log-Likelihood}_i = [\text{Adelie} \times \log(p_i)] + [(1 - \text{Adelie}) \times \log(1-p_i)]
$$

The log-likelihood of the entire dataset is just the sum of all the individual log-likelihood, since we are assuming independent observations, so we have

$$
\text{Log-Likelihood} = \sum_{i=1}^{n} \left[ [\text{Adelie} \times \log(p_i)] + [(1 - \text{Adelie}) \times \log(1-p_i)] \right]
$$

and now substituting in $p_i$ in terms of the parameters, we have
\begin{align}
\text{Log-Likelihood} &= \underbrace{\sum_{i=1}^{n} [[\text{Adelie} \times \log(\operatorname{expit} \left(\beta_0 + \beta_1 [\text{Bill Length}]_i + \beta_2 [\text{Body Mass}]_i \right))]}_{\text{Contribution from Adelie observations}} \\ &+ \underbrace{[(1 - \text{Adelie}) \times \log(1-\operatorname{expit} \left(\beta_0 + \beta_1 [\text{Bill Length}]_i + \beta_2 [\text{Body Mass}]_i \right))]}_{\text{Contribution from Gentoo observations}}]
\end{align}

We can then pick $\beta_0$, $\beta_1$, and $\beta_2$ to maximize this log-likelihood function, or as is often done in practice, minimize the negative log-likelihood function. We will need to do this with numerical methods, rather than obtaining an analytical solution with calculus, since no closed-form solution exists.

# Logistic regression with `glm()`

Before we implement logistic regression by hand, we will use the `glm()` function in R as a baseline. Under the hood, R uses the [Fisher Scoring Algorithm](https://en.wikipedia.org/wiki/Scoring_algorithm) to obtain the maximum likelihood estimates.

```{r}
# Fit the logistic regression model
model_glm <- glm(adelie ~ bill_length_cm + body_mass_kg,
                 family=binomial(link = "logit"),
                 data=df)
```

Now that we have fit the model, let's look at the predictions of the model. We can make a grid of covariate values, and ask the model to give us the predicted probability of Species = Adelie for each one.

```{r}
# Create a grid of values on which to evaluate probabilities
grid <-
crossing(bill_length_cm = seq(min(df$bill_length_cm)-0.2, max(df$bill_length_cm)+0.2, 0.001),
         body_mass_kg = seq(min(df$body_mass_kg)-0.1, max(df$body_mass_kg)+0.1, 0.005))
```

```{r}
# Take a look at the structure of `grid`
head(grid)
```

We use the `predict()` function to obtain the predicted probabilities. Using `type = "response"` specifies that we want the predictions of the probability scale (i.e., after passing the linear predictor through the `expit` function.).

```{r}
grid$predicted <- predict(model_glm, grid, type = "response")
```

Now that we have the predictions, let's plot them and overlay the data with their true labels. The model looks to be performing pretty well!

```{r}
#| column: page-right
#| fig-width: 12
grid %>%
  ggplot() +
  aes(x=bill_length_cm,
      y=body_mass_kg) +
  geom_raster(aes(fill=predicted)) +
  geom_point(data=df, mapping = aes(color=species)) +
  geom_point(data=df, color="black", shape=21) +
  scale_fill_viridis_c(breaks = seq(0, 1, 0.25),
                       limits=c(0,1)) +
  scale_color_brewer(palette="Dark2") +
  scale_x_continuous(breaks=seq(3, 6, 0.5)) +
  scale_y_continuous(breaks=seq(3, 6, 0.5)) +
  labs(fill = "Probability of Adelie\n",
       color = "Species",
       x = "Bill length (mm)",
       y = "Body mass (g)",
       title = "Visualizing the predictions of the logistic regression model") +
  theme(legend.key.height = unit(1, "cm"))
```


# Logistic regression with `optim()`

Now that we know what to expect after using `glm()`, let's implement logistic regression by hand.

Recall that we would like to numerically determine the beta values that minimize the negative log-likelihood. The `optim()` function in R is a general-purpose function for minimizing functions. This [short video](https://youtu.be/Qvye1wDa0kk) is a great introduction to `optim()`.

`optim()` has an algorithm called [Nelder-Mead](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) that searches the parameter space and converges on the minimum value. It is a direct search method that only requires the negative log-likelihood function as input. This animation demonstrates the Nelder-Mead algorithm in action^[Sourced from [here](https://commons.wikimedia.org/wiki/File:Nelder-Mead_Rosenbrock.gif)].

![Nelder-Mead animation](nelder_mead.mp4)]

To use `optim()`, we create a function that takes as input the parameters and returns the negative log-likelihood function. The below code is a translation of the mathematical notation from above.

```{r}
neg_loglikelihood_function <- function(parameters){
  
  # optim() expects the parameters as a single vector, so we set the coefficients
  # as the elements of a vector called `parameters`
  b0 <- parameters[1]
  b1 <- parameters[2]
  b2 <- parameters[3]
  
  linear_predictor <- (b0) + (b1*df$bill_length_cm) + (b2*df$body_mass_kg)
  
  # Likelihood for each observation
  # If the observation is Adelie, then the likelihood is the probability of Adelie
  # If the observation is not Adelie (i.e., Gentoo), then the likelihood is the probability of not Adelie
  # which is 1 - P(Adelie)
  likelihood <- ifelse(df$adelie==1,
                       expit(linear_predictor),
                       1-expit(linear_predictor))
  
  # Log-likelihood for each observation
  log_likelihood <- log(likelihood)
  
  # Joint log-likelihood for all the observations. Note the sum because
  # multiplication is addition on the log-scale
  total_log_likelihood <- sum(log_likelihood)
  
  # the optim() function only minimizes, so we return the negative log-likelihood
  # and then maximize it
  return(-total_log_likelihood)
}
```

As an example, we can pass in $\beta_0 = 1, \beta_1 = 2, \beta_2 = 3$ and see what the negative log-likelihood is.

```{r}
neg_loglikelihood_function(c(1,2,3))
```

Because it is very high, we know that these are poor choices for the parameter values. We can visualize the negative log-likelihood function for a variety of values. Since there are 3 parameters in our model, and we cannot visualize in 4D, we set $\beta_0 = 58.075$, which was the optimzed value found by `glm()` and we can visualize how the negative log-likelihood varies with $\beta_1$ and $beta_2$.

```{r}
# Create a grid of parameter values
grid <-
  crossing(
    b0 = 58.075,
    b1 = seq(-12, -7, length.out=1e2),
    b2 = seq(-6, -2, length.out=1e2)
  )
```

```{r}
# Evaluate the negative log-likelihood for each parameter value
grid$neg_loglikelihood <-
  pmap_dbl(grid,
           ~neg_loglikelihood_function(c(..1, ..2, ..3)))
```

```{r}
#| column: page-right
#| fig-width: 12
# Show a heatmap of the negative log-likelihood with contour lines
grid %>%
  ggplot() +
  aes(x=b1,
      y=b2) +
  geom_raster(aes(fill=neg_loglikelihood)) +
  geom_contour(aes(z=neg_loglikelihood), bins = 50, size=0.1, color="gray") +
  scale_fill_viridis_c() +
  scale_color_brewer(palette="Dark2") +
  annotate(geom="point", x=-8.999, y=-4.363, color="red") +
  labs(fill = "Negative log-likelihood\n",
       x = "Beta 1",
       y = "Beta 2",
       title = "Visualizing the negative log-likelihood function") +
  theme(legend.key.height = unit(1, "cm"))
```

To use `optim()`, we pass in the starting parameter values to `par` and the function to be minimized (the negative log-likelihood) to `fn`. Finally, we'll specify `method="Nelder-Mead"`.

```{r}
optim_results <-
  optim(par=c(0,0,0),                      # Initial values
        fn = neg_loglikelihood_function,   # Objective function to be minimized
        method="Nelder-Mead")              # Optimization method
```

The maximum likelihood estimates are stored in the `$par` attribute of the `optim` object

```{r}
optim_results$par
```

which we can compare with the coefficients obtained from `glm()`, and we see that they match quite closely.

```{r}
coef(model_glm)
```

The below animation demonstrates the path of the Nelder-Mead function. As stated above, for the purpose of the animation, we set the optimized value of $\beta_0 = 58.075$ and we can visualize how the negative log-likelihood is optimized with respect to $\beta_1$ and $beta_2$.

```{r}
#| show: false
if (!file.exists("nelder_mead_path.mp4")) {
  
  neg_loglikelihood_function <- function(parameters){
    
    # optim() expects the parameters as a single vector, so we set the coefficients
    # as the elements of a vector called `parameters`
    b0 <- 58.075
    b1 <- parameters[1]
    b2 <- parameters[2]
    
    linear_predictor <- (b0) + (b1*df$bill_length_cm) + (b2*df$body_mass_kg)
    
    # Likelihood for each observation
    # If the observation is Adelie, then the likelihood is the probability of Adelie
    # If the observation is not Adelie (i.e., Gentoo), then the likelihood is the probability of not Adelie
    # which is 1 - P(Adelie)
    likelihood <- ifelse(df$adelie==1,
                         expit(linear_predictor),
                         1-expit(linear_predictor))
    
    # Log-likelihood for each observation
    log_likelihood <- log(likelihood)
    
    # Joint log-likelihood for all the observations. Note the sum because
    # multiplication is addition on the log-scale
    total_log_likelihood <- sum(log_likelihood)
    
    # the optim() function only minimizes, so we return the negative log-likelihood
    # and then maximize it
    return(-total_log_likelihood)
  }
  
  run_opt <- function(maxit){
    set.seed(1234)
    res <-
    optim(par=c(-5, -5),
          fn = neg_loglikelihood_function,
          method="Nelder-Mead",
          control=list(maxit=maxit))
    
    return(tibble(b1=res$par[1],
                  b2=res$par[2],
                  val=res$value))
  }
  
  counts <- optim_res$counts["function"]
  
  path_df <-
    map_dfr(1:counts, ~run_opt(.x)) %>%
    mutate(step = 1:counts)
  
  grid <-
    crossing(
      b1 = seq(-12, -3, length.out=1e3),
      b2 = seq(-10, -3, length.out=1e3)
    )
  
  grid$neg_loglikelihood <-
    pmap_dbl(grid,
             ~neg_loglikelihood_function(c(..1, ..2)))

  p1 <-
  path_df %>%
    ggplot() +
    aes(x=b1, y=b2) +
    geom_raster(data=grid, aes(fill = neg_loglikelihood)) +
    geom_contour(data = grid, aes(z=neg_loglikelihood), bins = 50, size=0.1, color="gray") +
    geom_point(color="white") +
    geom_path(color="white", alpha=0.5) +
    scale_fill_viridis_c() +
    coord_cartesian(expand = FALSE) +
    annotate(geom="point", x=-8.999, y=-4.363, color="red") +
    transition_reveal(along = step) +
    labs(subtitle = "Iteration: {frame_along}",
         fill = "Negative log-likelihood\n",
         x = "Beta 1",
         y = "Beta 2",
        title = "Visualizing the path of Nelder-Mead optimization") +
    theme(legend.position = "none",
          legend.key.height = unit(1, "cm"))
  
  gif <- animate(p1,
                 duration=10,
                 height = 4,
                 width = 5,
                 units = "in",
                 res = 300,
                 renderer = ffmpeg_renderer())
  
  # Save to mp4
  anim_save(animation = gif, filename = "p1.mp4")
  
  p2 <-
  path_df %>%
    ggplot() +
    aes(x=step, y=val) +
    geom_point() +
    geom_path(alpha=0.5) +
    transition_reveal(along = step) +
    cowplot::theme_cowplot() +
    labs(subtitle = "Objective function",
         x="Iteration",
         y='Negative Log-Likelihood') +
    theme(legend.position = "none")
  
  gif <- animate(p2,
                 duration=10,
                 height = 4,
                 width = 3,
                 units = "in",
                 res = 300,
                 renderer = ffmpeg_renderer())
  
  # Save to mp4
  anim_save(animation = gif, filename = "p2.mp4")
  
  system('ffmpeg -i p1.mp4 -i p2.mp4 -filter_complex hstack nelder_mead_path.mp4 -vsync 2')
}
```

![Animation of the path taken by the Nelder Mead algorithm](nelder_mead_path.mp4){fig-width=100%}

# Logistic regression with gradient descent

Going one step further, instead of using a built-in optimization algorithm, let's maximize the likelihood ourselves using gradient descent. If you need a refresher, I have written a blog post on gradient descent which you can find [here](https://maximilianrohde.com/posts/gradient-descent-pt1/).

We need the gradient of the negative-log likelihood function. The slope with respect to the jth parameter is given by

\begin{align}
[\operatorname{expit}(\mathbf{\beta} \cdot \mathbf{x})-\mathbf{y}] \mathbf{x}_{j} \implies [\hat{\mathbf{y}}-\mathbf{y}] \mathbf{x}_{j}
\end{align}
so then the gradient can be written as
\begin{align}
\mathbf{X}^T [\operatorname{expit}(\mathbf{X} \mathbf{\beta}) - \mathbf{y}]
\end{align}
or equivalently
\begin{align}
\mathbf{X}^T (\hat{\mathbf{y}} - \mathbf{y})
\end{align}

You can find a nice derivation of the derivative of the negative log-likelihood for logistic regression [here](https://web.stanford.edu/~jurafsky/slp3/5.pdf).

Additionally, automatic differentiation can be used to obtain gradients for arbitrary functions, which is used heavily in deep learning. An example to do this in R using the `torch` library is shown [here](https://rgiordan.github.io/code/2022/04/01/rtorch_example.html).

We implement the above equations in the following function for the gradient.

```{r}
gradient <- function(parameters){
  # Given a vector of parameters values, return the current gradient
  
  b0 <- parameters[1]
  b1 <- parameters[2]
  b2 <- parameters[3]
  
  # Define design matrix
  X <- cbind(rep(1, nrow(df)),
                 df$bill_length_cm,
                 df$body_mass_kg)
  
  beta <- matrix(parameters)
  
  y_hat <- expit(X %*% beta)
  
  gradient <- t(X) %*% (y_hat - df$adelie)
  
  return(gradient)
}
```

::: {.column-margin}
We must specify `type="2"` in the `norm()` function to specify that we want the Euclidean length of the vector.
:::

Now we implement the gradient descent algorithm. We stop if the difference between the new parameter vector and old parameter vector is less than $10^{-6}$.

```{r}
#| cache: true

step_size <- 0.001
theta <- c(0,0,0)
iter <- 1

while (TRUE) {
  iter <- iter + 1
  current_gradient <- gradient(theta)
  
  theta_new <- theta - (step_size * current_gradient)
  
  if (norm(theta - theta_new, type="2") < 1e-6) {
    break
  } else{
    theta <- theta_new
  }
}

print(glue("Number of iterations: {iter}"))
print(glue("Final parameter values: {as.numeric(theta)}"))
print(glue("`glm()` parameter values (for comparison): {as.numeric(coef(model_glm))}"))
```

Again, we see that the results are very close to the `glm()` results.

# Conclusion

Hopefully this post was helpful for understanding the inner working of logistic regression and how the principles can be extended to other types of models. For example, Poisson regression is another type of generalized linear model just like logistic regression, where in that case we use the `exp` function instead of the `expit` function to constrain parameter values to lie in the range $[0, \infty]$.


